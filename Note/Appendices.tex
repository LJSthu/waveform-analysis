\appendix
\section{$\mu$ estimation bias of FBMP}
\label{sec:mubias}

\subsection{Model}

The model is described in section~\ref{subsec:fbmp}. 

\begin{equation}
\begin{aligned}
    \left.
    \begin{bmatrix}
        \bm{w} \\
        \bm{q}'
    \end{bmatrix}
    \right\vert\bm{z}
    &\sim \mathrm{Normal}\left(
    \begin{bmatrix}
        \bm{V}_\mathrm{PE}\bm{z} \\
        \bm{z}
    \end{bmatrix}, 
    \begin{bmatrix}
        \bm{\Sigma}_z & \bm{V}_\mathrm{PE}\bm{Z} \\
        \bm{Z}\bm{V}_\mathrm{PE}^\intercal & \bm{Z}
    \end{bmatrix}
    \right) \\
    \bm{\Sigma}_z &= \bm{V}_\mathrm{PE}\bm{Z}\bm{V}_\mathrm{PE}^\intercal+\sigma_\epsilon^2\bm{I}_M 
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
    \nu =& \log[p(\bm{w},\bm{z})] = \log[p(\bm{w}|\bm{z})p(\bm{z})] \\
    =& -\frac{1}{2}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})^\intercal\bm{\Sigma}_z^{-1}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})-\frac{1}{2}\log\det\bm{\Sigma}_z-\frac{N}{2}\log2\pi \\
    &+ \sum_{i}\log{\mathrm{Poisson}(z_i,p_i)}
\end{aligned}
\end{equation}

$\bm{V}_\mathrm{PE}$ is a $M\times N$ matrix, where $N=N_\mathrm{PE}$ is the simulation truth of PE number. 

\subsection{Approximation}

\begin{itemize}
    \item Poisson $\rightarrow$ Bernoulli, $z_i=0,\bm{Z}_{ii}=0$ or $z_i=1,\bm{Z}_{ii}=\sigma_q^2$
    \item SPE approximation
    \begin{itemize}
    \item For \emph{delta} approximation, $\bm{V}_{PE}=\begin{bmatrix}\bm{I} \\\bm{O}\end{bmatrix}$, $w_i\sim \mathrm{Normal}(1,\sigma_q^2+\sigma_\epsilon^2)$
    \item For \emph{flat} approximation, $\bm{V}_{PE}=\frac{1}{\sqrt{M}}\bm{J}_{M,N}$, $w_i\sim \mathrm{Normal}(\frac{N}{\sqrt{M}},\frac{N}{\sqrt{M}}\sigma_q^2+\sigma_\epsilon^2)$, $\sum_i^M w_i\sim \mathrm{Normal}(N\sqrt{M},M^{\frac{3}{2}}N\sigma_q^2+M\sigma_\epsilon^2)$
    \end{itemize}
    \item flat prior, $P(z_i=1)=\lambda,P(z_i=0)=1-\lambda$
\end{itemize}

The factor $\frac{1}{\sqrt{M}}$ of $\bm{V}_{PE}$ in flat approximation is required by FBMP's repeated greedy search (RGS). 

\subsection{Evaluation}

For a certain $(\bm{w}, \bm{z})$, let $K=\{i|z_i=1\}$, $|K|=n$, we evaluate the evolution process of $\nu$ in 2 approximations respectively. 

\subsubsection{Delta approximation}

\begin{equation}
\begin{aligned}
    \bm{\Sigma}_z &= \begin{bmatrix}
        \bm{Z} & \bm{O} \\
        \bm{O} & \bm{O}
    \end{bmatrix} + \sigma_\epsilon^2\bm{I}_M \\
    \det\bm{\Sigma}_z &= \Pi_i (z_i\sigma_q^2+\sigma_\epsilon^2) \\
    \bm{\Sigma}_{zij}^{-1} &= \begin{cases}
        \frac{1}{z_i\sigma_q^2+\sigma_\epsilon^2} & \text{ if } i=j \\ 
        0 & \text{ if } i\neq j 
    \end{cases}
\end{aligned}
\end{equation}

Thus, 

\begin{equation}
\begin{aligned}
    -2\nu =& \sum_{i\in K}\frac{(w_i - 1)^2}{\sigma_q^2+\sigma_\epsilon^2} + \sum_{i\in \bar{K}}\frac{w_i^2}{\sigma_\epsilon^2} \\ 
    &+ n\log(\sigma_q^2+\sigma_\epsilon^2) + (N-n)\log\sigma_\epsilon^2 - 2n\log\lambda - 2(N-n)\log(1-\lambda) + \mathrm{const}.
\end{aligned}
\end{equation}

Additionally, 

\begin{equation}
    \frac{w_i^2}{\sigma_\epsilon^2} = \frac{1}{\sigma_\epsilon^2}\left((\sigma_q^2+\sigma_\epsilon^2)\frac{(w_i - 1)^2}{\sigma_q^2+\sigma_\epsilon^2} + 2(w_i - 1) + 1\right).
\end{equation}

Some statistics we use below,

\begin{equation}
    \sum_{i\in K}\frac{(w_i - 1)^2}{\sigma_q^2+\sigma_\epsilon^2} \sim \chi^2(n).
\end{equation}

Calculate expectation on distribution of $\bm{w}$ and $\bm{z}$ with $|K|=n$, 

\begin{equation}
\begin{aligned}
    -2E(\nu)_n =& n + (N - n)\frac{\sigma_q^2+\sigma_\epsilon^2}{\sigma_\epsilon^2} + (N - n)\frac{1}{\sigma_\epsilon^2} \\ 
    &+ n\log(\sigma_q^2+\sigma_\epsilon^2) + (N-n)\log\sigma_\epsilon^2 - 2n\log\lambda - 2(N-n)\log(1-\lambda) + \mathrm{const}.
\end{aligned}
\end{equation}

Thus, 

\begin{equation}
\begin{aligned}
    -2D(\nu)_n =& -2E(\nu)_n - (-2E(\nu)_{n-1}) \\ 
    =& 1 - \frac{\sigma_q^2+\sigma_\epsilon^2}{\sigma_\epsilon^2} - \frac{1}{\sigma_\epsilon^2} + \log(\sigma_q^2+\sigma_\epsilon^2) - \log\sigma_\epsilon^2 - 2\log\lambda + 2\log(1-\lambda).
\end{aligned}
\end{equation}

Remember that $\sigma_q^2 \gg \sigma_\epsilon^2$ and $\lambda \ll 1$, set $x=\sigma_q^2/\sigma_\epsilon^2$, then,

\begin{equation}
    D(\nu)_n \approx \frac{1}{2}x - \frac{1}{2}\log x + \log\lambda
\end{equation}

while $x \gg 1$, $\frac{1}{2}x - \frac{1}{2}\log x \gg 1$, the sign of $D(\nu)_n$ is defined by the relative magnitude of $\frac{1}{2}x - \frac{1}{2}\log x$ and $\log\lambda$. 

\subsubsection{Flat approximation}

\begin{equation}
\begin{aligned}
    \bm{\Sigma}_z &= \frac{1}{M}\sigma_q^2\sum_i^N z_i\bm{J}_M + \sigma_\epsilon^2\bm{I}_M = \frac{\sigma_q^2n}{M}\bm{J}_M+\sigma_\epsilon^2\bm{I}_M \\
    \det\bm{\Sigma}_z &= (\sigma_\epsilon^2 + \sigma_q^2\sum_i^N z_i)\sigma_\epsilon^{2(M-1)} = (\sigma_\epsilon^2 + \sigma_q^2n)\sigma_\epsilon^{2(M-1)} \\
    \bm{\Sigma}_z^{-1} &= \frac{1}{\sigma_\epsilon^2}\left(\bm{I}_M - \frac{1}{M}\frac{\sigma_q^2\sum_i^N z_i}{\sigma_q^2\sum_i^N z_i+\sigma_\epsilon^2}\bm{J}_M\right) = \frac{1}{\sigma_\epsilon^2}\left(\bm{I}_M - \frac{1}{M}\frac{1}{1+\frac{\sigma_\epsilon^2}{\sigma_q^2n}}\bm{J}_M\right)
\end{aligned}
\end{equation}

set $M^\ast = M(1+\frac{\sigma_\epsilon^2}{\sigma_q^2n})$, regarding to $\sigma_q\gg\sigma_\epsilon$, thus $M^\ast\approx M$, and

\begin{equation}
    \bm{\Sigma}_z^{-1} = \frac{1}{\sigma_\epsilon^2}\left(\bm{I}_M - \frac{1}{M^\ast}\bm{J}_M\right).
\end{equation}

Thus, 

\begin{equation}
\begin{aligned}
    -2\nu =& \frac{1}{\sigma_\epsilon^2}\left[\sum_i^M\left(w_i-\frac{n}{\sqrt{M}}\right)^2-\frac{1}{M^\ast}\left(\sum_i^M w_i-n\sqrt{M}\right)^2\right] \\
    &+ \log(\sigma_q^2n+\sigma_\epsilon^2) - 2n\log\lambda - 2(N-n)\log(1-\lambda) + \mathrm{const}.
\end{aligned}
\end{equation}

Additionally, 

\begin{equation}
\begin{aligned}
    \left(w_i-\frac{n}{\sqrt{M}}\right)^2 =& \left(w_i - \frac{N}{\sqrt{M}}\right)^2 + 2\left(w_i - \frac{N}{\sqrt{M}}\right)\frac{N-n}{\sqrt{M}} + \frac{(N-n)^2}{M} \\
    \left(\sum_i^M w_i-n\sqrt{M}\right)^2 =& \left(\sum_i^M w_i-N\sqrt{M}\right)^2 + 2\left(\sum_i^M w_i-N\sqrt{M}\right)(N-n)\sqrt{M} + (N-n)^2M.
\end{aligned}
\end{equation}

Some statistics we use below,

\begin{equation}
\begin{aligned}
    \sum_i^M\frac{(w_i - \frac{N}{\sqrt{M}})^2}{\frac{N}{\sqrt{M}}\sigma_q^2+\sigma_\epsilon^2} &\sim \chi^2(M) \\
    \frac{(\sum_i^M w_i-N\sqrt{M})^2}{M^{\frac{3}{2}}N\sigma_q^2+M\sigma_\epsilon^2} &\sim \chi^2(1).
\end{aligned}
\end{equation}

Calculate expectation on distribution of $\bm{w}$ and $\bm{z}$ with $|K|=n$, 

\begin{equation}
\begin{aligned}
    -2E(\nu)_n =& \frac{1}{\sigma_\epsilon^2}\left[\left(N\sqrt{M}\sigma_q^2+(N-n)^2\right)\left(1-\frac{M}{M^\ast}\right)+\left(M-\frac{M}{M^\ast}\right)\sigma_\epsilon^2\right] \\
    &+ \log\left(\frac{\sigma_q^2}{\sigma_\epsilon^2}n+1\right) - 2n\log\lambda - 2(N-n)\log(1-\lambda) + \mathrm{const}.
\end{aligned}
\end{equation}

Thus, 

\begin{equation}
\begin{aligned}
    E(\nu)_n' =& -\frac{1}{2}\Biggl\{\frac{1}{\sigma_\epsilon^2}\Bigl[2(n-N)\left(1-\frac{M}{M^\ast}\right) \\
    &+\left(N\sqrt{M}\sigma_q^2+(N-n)^2+\sigma_\epsilon^2\right)\frac{M}{M^{\ast2}}M^{\ast'}\Bigr] +\frac{1}{n+\frac{\sigma_\epsilon^2}{\sigma_q^2}}\Biggr\} + \log\lambda - \log(1-\lambda)
\end{aligned}
\end{equation}
where $M^{\ast'}=-\frac{M\sigma_\epsilon^2}{\sigma_q^2n^2}$. 

When $n=N$,

\begin{equation}
\begin{aligned}
    E(\nu)_n'|_N =& \frac{1}{2}\frac{N}{\left(N+\frac{\sigma_\epsilon^2}{\sigma_q^2}\right)^2}(\sqrt{M}-1) + \log\lambda - \log(1-\lambda).
\end{aligned}
\end{equation}

Remember that $M \gg 1$, then,

\begin{equation}
    C = \frac{1}{2}\frac{N}{\left(N+\frac{\sigma_\epsilon^2}{\sigma_q^2}\right)^2}(\sqrt{M}-1) > 0
\end{equation}

the sign of $E(\nu)_n'|_N$ is defined by the relative magnitude of $C$ and $\log\lambda$. 

\subsection{Discussion}

When the time bin is very thin, $\log\lambda \ll 0$, $D(\nu)_n < 0$. When Bernoulli approximation fails, $D(\nu)_n > 0$. 

While we easonably assume $E(\nu)_n$ is near its maximum when $n=N_\mathrm{PE}$, considering the fact that RGS in FBMP has imperfect ergodicity, the bias is positive given $D(\nu)_n > 0$ (or $E(\nu)_n'|_N > 0$) as RGS does \emph{not} stop immediately when $n=N_\mathrm{PE}$. Given $D(\nu)_n < 0$ (or $E(\nu)_n'|_N < 0$), the bias is be negative. 
