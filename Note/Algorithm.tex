\section{Algorithms and their performance}
\label{sec:algorithm}

Waveform analysis is to obtain $t_i$ and $q_i$ estimators $\hat{t}_i$ and $\hat{q}_i$ from waveform $w(t)$, where the output indices $i$ are from 1 to $\hat{N}$ and $\hat{N}$ is an estimator of $N_\mathrm{PE}$ in eq.~\eqref{eq:lc-sample}. Figure~\ref{fig:io} illustrates the input $w(t)$ and the outputs $\bm{\hat{t}}, \hat{\bm{q}}$, where boldface $\hat{\bm{t}}$ denotes the vector $\hat{t}_i$. 
\begin{figure}[H]
  \centering
  \begin{subfigure}{.45\textwidth}
    \resizebox{\textwidth}{!}{\input{figures/wave.pgf}}
    \caption{\label{fig:input} Input $w(t)$ reproduced from figure~\ref{fig:pile}.}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
    \resizebox{\textwidth}{!}{\input{figures/charge.pgf}}
    \caption{\label{fig:output} Output $\hat{\bm{t}}, \hat{\bm{q}}$ obtained from \subref{fig:input}.}
  \end{subfigure}
  \caption{\label{fig:io} Waveform analysis takes PMT waveform input in~\subref{fig:input} and outputs times $\hat{\bm{t}}$ and charges $\hat{\bm{q}}$ of the PEs in~\subref{fig:output}. }
\end{figure}

The fluctuation of $q_i$ makes $\hat{t}_i$ ambiguous and $\hat{N}$ fail to estimate $N_\mathrm{PE}$. For example, 1, 2 and even 3~PEs can generate exactly the same charge as $1.6$.  A single PE charged $1$ may be mis-interpreted as 2~PEs at consecutive $\hat{t}_i$ and $\hat{t}_{i+1}$ with $\hat{q}_i=\hat{q}_{i+1}=0.5$.

\subsection{Evaluation criteria}
\label{sec:criteria}
In the darkness of the $t_i/q_i$ ambiguity, we introduce a set of evaluation criteria to assess the algorithms' performance.

\subsubsection{Kullback-Leibler divergence}
\label{sec:pseudo}

We construct a light curve estimator $\hat{\phi}(t)$ from $\bm{\hat{t}}$, $\bm{\hat{q}}$ and $\hat{N}$,
\begin{equation}
  \label{eq:lc}
  \hat{\phi}(t) = \sum_{i=1}^{\hat{N}} \hat{q}_i\delta(t-\hat{t}_i),
\end{equation}
which resembles eq.~\eqref{eq:lc-sample}.
Consider the non-normalized Kullback-Leibler~(KL) divergence~\cite{mihoko_robust_2002} between $\hat{\phi}(t)$ and $\phi(t-t_{0})$,
\begin{equation}
  \begin{aligned}
    D_\mathrm{KL}\left[\hat{\phi}(t) \parallel \phi(t-t_0)\right] & =\int \left[\hat{\phi}(t) \log\frac{\hat{\phi}(t)}{\phi(t-t_0)} + \phi(t-t_0) - \hat{\phi}(t) \right]\mathrm{d}t \\
    & = - \int \hat{\phi}(t) \log\phi(t-t_0)\mathrm{d}t + \int \left[\hat{\phi}(t) \log\hat{\phi}(t) + \phi(t) - \hat{\phi}(t) \right]\mathrm{d}t \\
    & = - \sum_{i=1}^{\hat{N}}\int \hat{q}_i\delta(t-\hat{t_i}) \log\phi(t-t_0)\mathrm{d}t + C \\
    & = -\log \prod_{i=1}^{\hat{N}} \left[\phi(\hat{t}_i-t_0)\right]^{\hat{q}_i} + C
  \label{eq:kl}
  \end{aligned}
\end{equation}
where $C$ is a constant regarding $t_0$.  Define the KL estimator as
\begin{equation}
  \begin{aligned}
  \label{eq:pseudo}
  \hat{t}_\mathrm{KL} &= \arg\underset{t_0}{\min}~D_\mathrm{KL}\left[\hat{\phi}(t) \parallel \phi(t-t_0)\right] \\
  &= \arg\underset{t_0}{\max} \prod_{i=1}^{\hat{N}} \left[\phi(\hat{t}_i-t_0)\right]^{\hat{q}_i},
  \end{aligned}
\end{equation}
which reduces to an MLE like eq.~\eqref{eq:2} if $\hat{q}_i\equiv 1$.  $\hat{t}_\mathrm{KL}$ estimates $t_0$ when $t_i, q_i, N_\mathrm{PE}$ are all uncertain.

Denoting $\Delta t_0$ as $\hat{t}_\mathrm{KL} - t_0$, the standard deviation $\sigma_\mathrm{KL}$ of $\Delta t_0$ on a batch of waveforms is the resolution of an algorithm.

\subsubsection{Residual sum of squares}
\label{sec:rss}

Following eqs.~\eqref{eq:1} and~\eqref{eq:lc}, we construct an estimator of a waveform,
\begin{equation}
  \label{eq:w-hat}
  \hat{w}(t) = \sum_{i=1}^{\hat{N}}\hat{q}_i V_\mathrm{PE}(t-\hat{t}_i) = \hat{\phi}(t) \otimes V_\mathrm{PE}(t).
\end{equation}
Residual sum of squares~(RSS) of $\hat{w}(t)$ from $w(t)$ is a $\ell_2$-metric inspired by the Gaussian noise term in \eqref{eq:1},
\begin{equation}
  \label{eq:rss}
  \mathrm{RSS} \coloneqq\int\left[\hat{w}(t) - w(t)\right]^2\mathrm{d}t.
\end{equation}

Figure~\ref{fig:l2} demonstrates that if two functions donot overlap, the RSS of them remains constant no matter what their relative positions are.  The sampled light curves $\hat{\phi}(t)$ and $\tilde{\phi}(t)$ consist with delta functions and hardly overlap, therefore the RSS of them is useless.  Futhermore, RSS cannot compare a discrete function with a continuous one.  We shall only consider the RSS of waveforms.

\begin{figure}[H]
  \centering
  \resizebox{\textwidth}{!}{\input{figures/tab.pgf}}
  \caption{\label{fig:l2} REPLACE THIS FIGURE!  The RSS of red and blue curves is a function of the two shaded regions. It is a constant when the curves shifts horizontally provided they donot overlap.  In contract, the Wasserstein distance $D_w$ of the two curves is the distance between them.  It complements RSS and offers a time-sensitive metric suitable for the sparse PE space.}
\end{figure}

\subsubsection{Wasserstein distance}
\label{sec:W-dist}

\input{fom}

We shall evaluate the performance of waveform analysis algorithms ranging from heuristics, deconvolution, neuron network to regression, by the discussed criteria in this section.

\subsection{Heuristic methods}
By directly extracting waveform features, heuristic methods are straightforward and widely used. 

\subsubsection{Waveform shifting}
\label{sec:shifting}
By \textit{waveform shifting}, we extract more time information than TDC in a similar manner: select all the $t_i$ where the waveform $w(t_i)$ exceeds a threshold to suppress noise, shift them by $\Delta t$ according to the shape of single PE response $V_\mathrm{PE}(t)$ in eq.~\eqref{eq:dayaspe} to get $\hat{t}_i$, then take $\hat{q}_i \propto w(\hat{t}_i)$ so that the total charge $\sum \hat{q}_i$ aligns with the integration of the waveform $w(t)$.  Though the $\hat{q}_i$'s obtained are smaller than PE charges as in figure~\ref{fig:shifting}, waveform shifting does minimal analysis and serves as a baseline method.

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/threshold.pgf}}
    \caption{\label{fig:shifting} A waveform shifting example gives \\ $\Delta t_0=\SI{10.74}{ns}$, $\mathrm{RSS}=\SI{4805.4}{mV^2}$, $D_w=\SI{8.72}{ns}$.}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/findpeak.pgf}}
    \caption{\label{fig:peak} A peak finding example gives \\ $\Delta t_0=\SI{-1.93}{ns}$, $\mathrm{RSS}=\SI{317.3}{mV^2}$, $D_w=\SI{2.40}{ns}$.}
  \end{subfigure}
  \caption{Demonstrations of heuristic methods on a waveform sampled from $\mu=4$, $\tau_l=\SI{20}{ns}$, $\sigma_l=\SI{5}{ns}$ light curve conditions.  Peak finding in~\subref{fig:peak} handles charges more realistically than waveform shifting in~\subref{fig:shifting}, giving better numbers in the three criteria.}
\end{figure}

\subsubsection{Peak finding}
\label{sec:findpeak}

\textit{Peak finding} is more effective than waveform shifting by exploiting PE pulses.  We smooth the waveforms by a low-pass Savitzky-Golay filter~\cite{savitzky_smoothing_1964} and find all the peaks at $t_i$.  The following resembles waveform shifting: apply a shift to get $\hat{t}_i$, take $\hat{q}_i \propto w(\hat{t}_i)$ and normalize to the waveform integration. As shown in Figure~\ref{fig:peak}, peak finding output charges close to 1 and works well for lower PE counts.  But when PEs pile up closely, peaks overlap intensively, making this method unreliable.

\subsection{Deconvolution}
\label{sec:deconv}
Deconvolution methods do more than heuristic ones by using the full shape of $V_\mathrm{PE}(t)$, thus can accommodate overshoots and pile-ups.  Smoothing is necessary because deconvolution does not model the white noise.

\subsubsection{Fourier deconvolution}
\textit{Fourier deconvolution} is an option considered by JUNO prototype~\cite{zhang_comparison_2019}. The deconvolution relation is evident in the Fourier transform $\mathcal{F}$ to eq.~\eqref{eq:1},
\begin{equation}
  \label{eq:fourier}
  \begin{aligned}
  \mathcal{F}[w] & = \mathcal{F}[\tilde{\phi}]\mathcal{F}[V_\mathrm{PE}] + \mathcal{F}[\epsilon]\\
  \implies \mathcal{F}[\tilde{\phi}] & = \frac{\mathcal{F}[w]}{\mathcal{F}[V_\mathrm{PE}]} - \frac{\mathcal{F}[\epsilon]}{\mathcal{F}[V_\mathrm{PE}]}.
  \end{aligned}
\end{equation}
By low-pass filtering the waveform $w(t)$ to get $\tilde{w}(t)$, we suppress the noise term $\epsilon$, take the inverse Fourier transform $\hat{\phi}_1(t) = \mathcal{F}^{-1}\left[\frac{\mathcal{F}[\tilde{w}]}{\mathcal{F}[V_\mathrm{PE}]}\right](t)$, and compute $\hat{\phi}(t)$ as the over-threshold $q_\mathrm{th}$ part of $\hat{\phi}_1(t)$,
\begin{equation}
  \label{eq:fdconv2}
    \hat{\phi}(t) = \hat{\alpha}\underbrace{\hat{\phi}_1(t) I(\hat{\phi}_1(t) - q_\mathrm{th})}_{\text{over-threshold part of} \hat{\phi}_1(t)}  
\end{equation}
where $I(x)$ is the indicator function, and $\hat{\alpha}$ is a scaling factor to minimize RSS,
\begin{equation*}
  \begin{aligned}
  \label{eq:id}
  I(x) = \left\{
    \begin{array}{ll}
      1 & \mbox{, if $x\ge0$}, \\
      0 & \mbox{, otherwise}
    \end{array}
    \right.
    \quad~~~
    \hat{\alpha} = \arg \underset{\alpha}{\min}\mathrm{RSS}\left[\alpha\hat{\phi}(t)\otimes V_\mathrm{PE}(t),w(t)\right]. \\
  \end{aligned}
\end{equation*}

Figure~\ref{fig:fd} illustrates that fourier deconvolution outperforms heuristic methods.  Meanwhile, noise and precision loss in the waveform lead to smaller and even negative $\hat{q}_i$. Those can be mitigated by thresholding and scaling in eq.~\eqref{eq:fdconv2}, but calls for a more elegant solution.

\begin{figure}[H]
  \begin{subfigure}{0.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/fftrans.pgf}}
    \caption{\label{fig:fd} A Fourier deconvolution example: \\ $\Delta t_0=\SI{-1.16}{ns}$, $\mathrm{RSS}=\SI{124.7}{mV^2}$, $D_w=\SI{2.03}{ns}$.}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/lucyddm.pgf}}
    \caption{\label{fig:lucy} A Richardson-Lucy direct demodulation example:\\ $\Delta t_0=\SI{-0.75}{ns}$, $\mathrm{RSS}=\SI{70.3}{mV^2}$, $D_w=\SI{1.10}{ns}$.}
  \end{subfigure}
  \caption{Demonstrations of deconvolution methods on a waveform sampled from $\mu=4$, $\tau_l=\SI{20}{ns}$, $\sigma_l=\SI{5}{ns}$ light curve conditions. Richardson-Lucy direct demodulation in~\subref{fig:lucy} imposes positive charges in iterations and obtains better results than Fourier deconvolution in~\subref{fig:fd}.}
\end{figure}

\subsubsection{Richardson-Lucy direct demodulation}

\textit{Richardson-Lucy direct demodulation}~(LucyDDM)~\cite{lucy_iterative_1974} with a non-linear iteration to calculate deconvolution has a wide application in astronomy~\cite{li_richardson-lucy_2019} and image processing. We view $V_{\mathrm{PE}*}(t-s)$ as a conditional probability distribution $p(t|s)$ where $t$ denotes PMT amplified electron time and $s$ denotes the given PE time. By the Bayesian rule,
\begin{equation}
  \label{eq:lucy}
  \tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s) = \tilde{\phi}_*(s)p(t|s) = p(t,s) = \tilde{w}_*(t)p(s|t)
\end{equation}
where $p(t, s)$ is the joint distribution of amplified electron $t$ and PE time $s$, and $\tilde{w}$ is the smoothed $w$.  Cancel out the normalization factors,
\begin{equation}
  \label{eq:ptt}
  p(s|t) = \frac{\tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s)}{\tilde{w}_*(t)} = \frac{\tilde{\phi}(s) V_{\mathrm{PE}}(t-s)}{\int\tilde{\phi}(s') V_{\mathrm{PE}}(t-s')\mathrm{d}s'}.
\end{equation}
We construct $\phi_*$ recurrence relation,
\begin{equation}
  \label{eq:iter}
  \begin{aligned}
    \tilde{\phi}_*(s) & = \int p(s|t) \tilde{w}_*(t)\mathrm{d}t = \int \frac{\tilde{\phi}(s) V_{\mathrm{PE}}(t-s)}{\int\tilde{\phi}(s') V_{\mathrm{PE}}(t-s')\mathrm{d}s'} \tilde{w}_*(t) \mathrm{d}t \\
    \implies \hat{\phi}^{n+1}(s) & = \int \frac{\hat{\phi}^n(s) V_{\mathrm{PE}*}(t-s)}{\int\hat{\phi}^n(s') V_{\mathrm{PE}}(t-s')\mathrm{d}s'} \tilde{w}(t) \mathrm{d}t,
  \end{aligned}
\end{equation}
where only $V_{\mathrm{PE}*}$ in the numerator is normalized and superscript $n$ denotes the iteration step.
Like Fourier deconvolution in eq.~\eqref{eq:fdconv2}, we threshold and scale the converged $\hat{\phi}^\infty$ to get $\hat{\phi}$.  As shown in figure~\ref{fig:lucy} the positive constraint of $\hat{\phi}$ makes LucyDDM more resilient to noise.

The remaining noise in the smoothed $\tilde{w}$ crucially influence deconvolution.  A probablistic method will correctly model the noise term $\epsilon$, as we shall see in section \ref{sec:regression}.  Before that let's try something different.

\subsection{Convolutional neural network}

% Network structure
The advances in neural networks have brought breakthroughs in various domains like computer vision~\cite{he_deep_2015} and natural language processing~\cite{vaswani_attention_2017}. As an efficient composition of weighted additions and non-linear functions, neural networks have prevailed against many traditional algorithms. We introduce a multi-layered convolutional neural network~(CNN) to process $\hat{\phi}(t)$ from waveforms $w(t)$ in \eqref{eq:1}. 

Although advanced structures like residual connection and strategies like batch-norm alleviated training settle problems for deep networks, the pulse-shape and universality of $V_\mathrm{PE}(t)$ for all the PE allow only a few layers to recognize the patterns. We choose a shallow network structure of 5 layers~(figure~\ref{fig:struct}). Because $V_\mathrm{PE}(t)$ is localized, the convolutional widths are selected accordingly. 

The workflow of data processing consists of training and predicting. We find a mapping from waveform $w(t)$ to PE $\tilde{\phi}(t)$ with backpropagation method and supervised learning. We build the loss as the Wasserstein distance $D_w$ between the truth $\tilde{\phi}$ and predicted $\hat{\phi}(t)$ and minimize the loss when training. As discussed in section~\ref{sec:W-dist}, $D_w$ is chosen to handle the PE sparsity in training iterations. Figure~\ref{fig:loss} shows the evolution of Wasserstein distance during training. 

\begin{figure}[H]
  \begin{subfigure}{0.35\textwidth}
    \centering
    \begin{adjustbox}{width=0.65\textwidth}
      \input{model}
    \end{adjustbox}
    \caption{\label{fig:struct} CNN structure}
  \end{subfigure}
  \begin{subfigure}{0.6\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/epoch.pgf}}
    \caption{\label{fig:loss} Evolution of loss}
  \end{subfigure}
  \caption{\label{fig:CNN} Training process of a CNN. A shallow network structure of 5 layers is in~\subref{fig:struct} and the evolution of Wasserstein distance during training is in~\subref{fig:loss}.}
\end{figure}

Since $D_w$ disregards normalization, $\hat{\phi}$ is scaled to $\hat{\alpha}\hat{\phi}$ following~\eqref{eq:fdconv2}. Figure~\ref{fig:cnn} shows a demonstration with $D_w = \SI{0.64}{ns}$. 

The results of $\hat{\phi}(t)$ with $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$ are shown in figure~\ref{fig:cnn-npe}. The left panel is the box plot which shows the $D_w$ percentile vs $N_{\mathrm{PE}}$, and the right panel is the overall histogram of $D_w$. 

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/takarachargestats.pgf}}
    \caption{\label{fig:cnn-npe} $D_w$ vs $N_{\mathrm{PE}}$ Histogram \& Box plot}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/takara.pgf}}
    \caption{\label{fig:cnn}$\mathrm{RSS}=\SI{10.0}{mV^2},D_w=\SI{0.64}{ns},\Delta t_0=\SI{-3.05}{ns}$}
  \end{subfigure}
  \caption{CNN results when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$ show in~\subref{fig:cnn-npe}. The horizontal axis of the right subplot is in arbitrary unit. A demonstration shows in~\subref{fig:cnn}.}
\end{figure}

\subsection{Regression analysis}
\label{sec:regression}
With the generative waveform model in~\eqref{eq:1}, regression is the best for waveform analysis. Although computational complexity hinders the applications of regression in vast volumes of raw data, the advancement of big data infrastructure and sparsity research make the advantage of regression more pronounced.

We replace $\hat{N}$ with a fixed sample size $N_s$ and $\hat{t}_i$ with a fixed vector of times $t'_i$ in equation~\ref{eq:w-hat} and obtain, 
\begin{equation}
  \label{eq:gd}
  w'(t) = \sum_{i=1}^{N_s}q'_iV_\mathrm{PE}(t-t'_i).
\end{equation}

When $\{t'_i\}$ is dense enough, $\{\hat{q}_i\}$ determines the inferred PE distribution $\hat{\phi}(t)$,
\begin{equation}
  \label{eq:gd-phi}
  \hat{\phi}(t) = \sum_{i=1}^{N_s}\hat{q}_i\delta(t-t'_i).
\end{equation}

We have attempted to replace the dense $\bm{t'}$ grid in equation~\eqref{eq:gd} with a length-varying vector of sparse PEs. However, the truth $N_\mathrm{PE}$ is unknown, which prevents formulating an explicit trans-dimensional model. 

Because the output $\hat{\phi}(t)$ from a deconvolution method~(section~\ref{sec:deconv}) covers locations of non-zero $\bm{q}'$, it can initialize the $\bm{t}'$ grid to reduce $N_s$ with discretization and consequently the time consumption. 

\subsubsection{Gradient descent}

We construct estimators $\hat{q}_i$ by minimizing RSS of $w'(t)$ and $w(t)$, 
\begin{equation}
  \label{eq:gd-q}
  \bm{\hat{q}} = \arg \underset{\bm{q'}}{\min} \mathrm{RSS}\left[w'(t),w(t)\right].
\end{equation}

We realize an optimizer by a limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm with a bound constraint~\cite{byrd_limited_1995}. It imposes constraints that $q'_i > 0$. However, because this method minimizes RSS, we do not scale the converged $\bm{\hat{q}}$. The demonstration~\ref{fig:fitting} here shows a result with $D_w = \SI{0.68}{ns}$.

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/xiaopeipchargestats.pgf}}
    \caption{\label{fig:fitting-npe} $D_w$ vs $N_{\mathrm{PE}}$ Box plot \& Histogram}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/xiaopeip.pgf}}
    \caption{\label{fig:fitting}$\mathrm{RSS}=\SI{7.63}{mV^2},D_w=\SI{0.68}{ns},\Delta{t_0}=\SI{-3.22}{ns}$}
  \end{subfigure}
  \caption{Gradient descent results when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$ show in~\subref{fig:fitting-npe}. The horizontal axis of the right subplot is in arbitrary unit. A demonstration shows in~\subref{fig:fitting}.}
\end{figure}

The result of $\hat{q}$ when $\mu=4, \tau_l=\SI{20}{ns}$, and $\sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:fitting-npe}. The left panel is the box plot which shows the $D_w$ percentile vs $N_{\mathrm{PE}}$, and the right panel is the overall histogram of $D_w$. 

\subsubsection{Markov chain Monte Carlo}
\label{subsec:mcmc}
The optimization problem in equation~\eqref{eq:gd-q} is generally not convex and sticks in local minima. Markov Chain Monte Carlo~(MCMC) has the potential to overcome this difficulty.

Hamiltonian Monte Carlo~(HMC) is an efficient MCMC for high dimensional distributions~\cite{neal_mcmc_2012} such as the posterior of charge $\bm{q'}$. The posterior is combined with a $\bm{q'}$ prior conditioned on $t_0$ to form a hierarchical model, 
\begin{equation}
  \begin{aligned}
    t_{0} &\sim \mathrm{Uniform}(0, \overline{t_0}) \\
    p_i &= \mu \int_{t'_i-\frac{\Delta t'}{2}}^{t'_i+\frac{\Delta t'}{2}} \phi(t' - t_0)\mathrm{d}t' \approx \mu\phi(t'_i - t_0)\Delta{t'} \\
    z_i &\sim \mathrm{Binomial}(p_i) \\
    q'_{i,0}&=0\\
    q'_{i,1}& \sim \mathrm{Normal}(1, \sigma_q)\\
    q'_i &= q'_{i,z_i}\\
    w'(t) & = \sum_{i=1}^{N_s}q'_iV_\mathrm{PE}(t-t'_i)\\
    w(t) &\sim \mathrm{Normal}(w'(t), \sigma_\epsilon)
  \end{aligned}
  \label{eq:mixnormal}
\end{equation}
where $t_{0}$ is sampled from a uniform prior with an upper bound $\overline{t_{0}}$, and $p_i$ the expection of a PE hitting $(t_{i} - \frac{\Delta t'}{2}, t_{i} + \frac{\Delta t'}{2})$ when $\Delta t'$ is small enough. $q'_i$ is a mixture of 0 (no PE) and normally distributed $q'_{i,1}$ (1 PE). The inferred waveform $w'(t)$ differs from observable $w(t)$ by a white noise $\epsilon(t) \sim \mathrm{Normal}(0, \sigma_\epsilon)$. When $\Delta{t'} \to 0$, equation~\eqref{eq:mixnormal} is equivalent to equation~\eqref{eq:1} up to a shift by $t_0$. Instead of using $N_\mathrm{PE}$ to govern the number of pairs $(t_i, q_i)$, we encode the trans-dimensionality implicitly by $z_i$ to turn on or off $q'_i$. 

We generate posterior samples of $t_0$ and $\bm{q'}$ by HMC. Ergodicity of HMC prevents a local maxima from trapping the sampler. We use NumPyro~\cite{phan2019composable}, a graceful probabilistic programming library, to drive the HMC. 

We construct $\hat{t}_0$ and $\hat{q}_i$ as the mean estimators of posterior samples. Unlike the pseudo-likelihood $\hat{t}_\mathrm{KL}$ discussed in section~\ref{sec:pseudo}, $\hat{t}_0$ is a genuine Bayesian estimator. We construct $\hat{\phi}(t)$ as, 
\begin{equation}
  \label{eq:mcmc-phi}
  \hat{\phi}(t) = \sum_{i=1}^{N_s}\hat{q}_i\delta(t-t'_i).
\end{equation}

$D_w$ with $\mu=4, \tau_l=\SI{20}{ns}$, and $\sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:mcmc-npe}.
\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/mcmcchargestats.pgf}}
    \caption{\label{fig:mcmc-npe} $D_w$ vs $N_{\mathrm{PE}}$ Box plot \& Histogram}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/mcmc.pgf}}
    \caption{\label{fig:mcmc}$\mathrm{RSS}=\SI{16.25}{mV^2},D_w=\SI{0.76}{ns},\Delta{t_0}=\SI{-2.48}{ns}$}
  \end{subfigure}
  \caption{MCMC results when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$ show in~\subref{fig:mcmc-npe}. The horizontal axis of the right subplot is in arbitrary unit. A demonstration shows in~\subref{fig:mcmc}.}
\end{figure}

\subsubsection{Fast Bayesian Matching Pursuit}
\label{subsec:fbmp}
We find that $w(t)$ is a Gaussian process subject to $\bm{z}$,
\begin{equation}
    w(t)|\bm{z} \sim \mathcal{GP}\left[\sum_{i|z_i=1}V_\mathrm{PE}(t-t'_i), \sigma_q^2 \sum_{i|z_i=1}V_\mathrm{PE}(t'_i-t)V_\mathrm{PE}(s-t'_i) + \sigma_\epsilon^2 \right],
\end{equation}
where $\bm{z}$ controls which $t'_i$ is in use. In this method, we discretize the Gaussian process as a multivariate normal distribution together with $\bm{q}'$, 
\begin{equation}
\label{eq:mgauss}
\begin{aligned}
    \left.
    \begin{bmatrix}
        \bm{w} \\
        \bm{q}'
    \end{bmatrix}
    \right\vert\bm{z}
    &\sim \mathrm{Normal}\left(
    \begin{bmatrix}
        \bm{V}_\mathrm{PE}\bm{z} \\
        \bm{z}
    \end{bmatrix}, 
    \begin{bmatrix}
        \bm{\Sigma}_z & \bm{V}_\mathrm{PE}\bm{Z} \\
        \bm{Z}\bm{V}_\mathrm{PE}^\intercal & \bm{Z}
    \end{bmatrix}
    \right) \\
    \bm{\Sigma}_z &= \bm{V}_\mathrm{PE}\bm{Z}\bm{V}_\mathrm{PE}^\intercal+\sigma_\epsilon^2\bm{I}
\end{aligned}
\end{equation}
where $\bm{Z}$ is the diagonal matrix controlling the spread of $q'_i$. $\bm{I}$ is a unit matrix. Here $\bm{z}$ is not the same as that in section~\ref{subsec:mcmc}: 

\begin{equation}
  \begin{aligned}
    \mu_i &= \mu \int_{t'_i-\frac{\Delta t'}{2}}^{t'_i+\frac{\Delta t'}{2}} \phi(t' - t_0)\mathrm{d}t' \\
    z_i &\sim \mathrm{Poisson}(\mu_i).
  \end{aligned}
  \label{eq:poissonfbmp}
\end{equation}

A set $\mathcal{Z}$ contains infinite combinations of $\bm{z}$. But only a few $\bm{z}$'s dominate in $\mathcal{Z}$, leading to the Fast Bayesian Matching Pursuit~(FBMP), a parameter estimation method for sparse linear models~\cite{schniter_fast_2008}. FBMP defines a targeted subset $\mathcal{Z}'$ of $\mathcal{Z}$ with only the $\bm{z}$ giving non-negligible $p(\bm{w}|\bm{z})$. We approximate the posterior of $\bm{z}$ as 
\begin{equation}
    \label{eq:z}
    \begin{aligned}
    p(\bm{z}|\bm{w}) &= \frac{p(\bm{w}|\bm{z})p(\bm{z})}{\sum_{\bm{z}'\in\mathcal{Z}}p(\bm{w}|\bm{z'})p(\bm{z'})} \\
    &\approx \frac{p(\bm{w}|\bm{z})p(\bm{z})}{\sum_{\bm{z}'\in\mathcal{Z}'}p(\bm{w}|\bm{z'})p(\bm{z'})}.
    \end{aligned}
\end{equation}
A Poisson distributions of binned light curve gives the prior $p(\bm{z})$, 
\begin{equation}
\begin{aligned}
    p(\bm{z}|t_0, \mu) &= \prod_{i}\mathrm{Poisson}(z_i,\mu_i)
\end{aligned}
\end{equation}
where $\mu$ is the prior expected number of PEs and satisfies the normalization condition in equation~\eqref{eq:normal}. 
\begin{equation}
\label{eq:normal}
\begin{aligned}
  \sum_i \mu_i &= \mu
\end{aligned}
\end{equation}

The logarithm numerator of \eqref{eq:z}
\begin{equation}
    \label{eq:metric}
    \begin{aligned}
        \nu =& \log[p(\bm{w},\bm{z})] = \log[p(\bm{w}|\bm{z})p(\bm{z})] \\
        =& -\frac{1}{2}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})^\intercal\bm{\Sigma}_z^{-1}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})-\frac{1}{2}\log\det\bm{\Sigma}_z-\frac{N_s}{2}\log2\pi \\
        & +\sum_{i}\log{\mathrm{Poisson}(z_i,\mu_i)}
    \end{aligned}
\end{equation}
is a \textit{model selection metric}. A repeated greedy search (RGS) guided by equation~\eqref{eq:metric} is performed to construct the target set $\mathcal{Z}'$. Additionally, a linear algebraic iteration reduces the complexity of $p(\bm{z}|\bm{w})$ calculation~\cite{schniter_fast_2008}. 

For each $\bm{z} \in \mathcal{Z}'$, from \eqref{eq:mgauss} the expectation estimation of $\bm{q}'$ is, 
\begin{align}
    \hat{\bm{q}}_z = E(\bm{q}'|\bm{w},\bm{z}) &= \bm{z} + \bm{Z}\bm{V}_\mathrm{PE}^\intercal\bm{\Sigma}_z^{-1}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})
    \label{eq:fbmpcharge}
\end{align}

Finally, we collect all the $\bm{z} \in \mathcal{Z}'$ and their corresponding posterior probabilities $p(\bm{z}|\bm{w})$, as well as estimation $\hat{\bm{q}}_z$, to be the final posterior distribution. $\hat{\bm{z}}$ is the PE number in each time bin $\hat{\bm{t}}$. $\hat{\bm{q}}$, which is the total charge in each time bin, is estimated by the maximum posterior probability, 
\begin{equation}
    \begin{aligned}
        \label{eq:zposterior}
        \hat{\bm{z}} &= \arg \underset{\bm{z}}{\max} p(\bm{z}|\bm{w}) \\
        \hat{\bm{q}} &= \hat{\bm{q}}_{\hat{z}} \\
        \hat{w}(t) &= \sum_{i} \hat{q}_i V_\mathrm{PE}(t-t'_i)
    \end{aligned}
\end{equation}

Figure~\ref{fig:fbmp} shows one of the FBMP results. We can see that the original waveform $w(t)$ and the reconstructed waveform $\hat{w}(t)$ are very similar. The reconstructed and truth $\bm{q}$ are also similar. 

\begin{figure}[H]
    \centering
    \resizebox{0.6\textwidth}{!}{\input{figures/demoe2c0.pgf}}
    \caption{\label{fig:fbmp} FBMP Demo, $\mathrm{RSS}=\SI{15.8}{mV^2},D_w=\SI{0.59}{ns},\Delta{t_0}=\SI{-3.51}{ns}$}
\end{figure}

The result of $\hat{q}$ with $\mu=4, \tau_l=\SI{20}{ns}$, and $\sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:fbmp-npe}. The left panel is the box plot which shows the $D_w$ percentiles vs $N_{\mathrm{PE}}$, and the right panel is the overall histogram of $D_w$. 

\begin{figure}[H]
    \centering
    \resizebox{0.5\textwidth}{!}{\input{figures/fbmpchargestats.pgf}}
    \caption{\label{fig:fbmp-npe} $D_w$ vs $N_{\mathrm{PE}}$ Box plot \& Histogram, $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$, method is FBMP. The horizontal axis of the right subplot is in arbitrary unit}
\end{figure}

The denominator of equation~\eqref{eq:z} is actually the evidence $p(\bm{w}|t_0, \mu)$, which provides a superior estimator of $\mu$ and $t_0$ to equation~\eqref{eq:pseudo},
\begin{equation}
\begin{aligned}
    \label{eq:bayesianinter}
    \left(\hat{t}_0, \hat{\mu}\right) &= \arg\underset{t_0,\mu}{\max} p(\bm{w} | t_0, \mu) \\
    p(\bm{w}|t_0, \mu) &= \sum_{\bm{z}'\in\mathcal{Z}'}p(\bm{w}|\bm{z}',t_0,\mu)p(\bm{z}'|t_0,\mu) \\
    &= \sum_{\bm{z}'\in\mathcal{Z}'}p(\bm{w}|\bm{z}')p(\bm{z}'|t_0,\mu).
\end{aligned}
\end{equation}
where $p(\bm{w}|z', t_0, \mu) = p(\bm{w}|z')$ because $\bm{w}$ does not depend on $t_0$ or $\mu$ if $z'$ is given. If $t_0$ and $\mu$ are predicted by an event model $p(t_0, \mu | \mathcal{E})$, we can extend FBMP to the event reconstruction, bypassing the need to save PE $\bm{t}'$ and $\bm{q}'$. While FBMP provides the Bayesian interface of waveform analysis and event reconstruction, it is however out of the scope of this work and will be covered elsewhere. 

\subsection{Performance}

% Methods

Figure~\ref{fig:chargesummary} shows the $D_w$ summary of all methods with $\tau_l=\SI{20}{ns}$, and $\sigma_l=\SI{5}{ns}$. The error bars show 5 to 95 percentile of Wasserstein distance distributions. 

\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{\input{figures/summarycharge.pgf}}
    \caption{\label{fig:chargesummary} $D_w$ for all the methods with light curve $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$}
\end{figure}

It is apparent that FindPeak, Shift, and FFT are second-rated among all eight methods. On time resolution and charge reconstruction which we discuss in sections~\ref{subsec:timeresolution} and~\ref{subsec:chargereconstruction}, MCMC does not perform well and is time-consuming. So we only discuss the other four methods below. 

% Figure of Merits

The $D_w$ summary of four methods (LucyDDM, Fitting, CNN, FBMP) is plotted in figure~\ref{fig:wdistsummary}, with all dataset parameters combination. The error bar shows 5 to 95 percentile of Wasserstein distance distribution. Wasserstein distances of all 4 methods are approaching each other when $\mu$ increases. 

We can see the Wasserstein distance of method CNN is the smallest, as it directly uses Wasserstein distance as the loss during training. While the Fitting method treats RSS as its loss, FBMP obtains the least RSS among four methods (see figure~\ref{fig:rsssummary}). 
\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{\input{figures/vs-wdist.pgf}}
    \caption{\label{fig:wdistsummary} $D_w$ of methods, error bar 5--95 percentile ($\hat{q}$)}
\end{figure}

\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{\input{figures/vs-rss.pgf}}
    \caption{\label{fig:rsssummary} RSS of methods, error bar 5--95 percentile ($\hat{q}$)}
\end{figure}
