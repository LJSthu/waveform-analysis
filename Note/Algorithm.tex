\section{Algorithms and performance}
\label{sec:algorithm}

The goal of waveform analysis is to construct estimators $\hat{t}_i$ and $\hat{q}_i$ from waveform $w(t)$, in which the output indices $i$ is from 1 to $\hat{N}$.  Figure~\ref{fig:io} illustrates the input $w(t)$ and the output $\bm{\hat{t}}, \bm{\hat{q}}$, where the bold face $\bm{\hat{t}}$ denotes the vector $\hat{t}_i$. 
\begin{figure}[H]
  \centering
  \begin{subfigure}{.45\textwidth}
    \resizebox{\textwidth}{!}{\input{figures/wave.pgf}}
    \caption{\label{fig:input} Input $w(t)$}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
    \resizebox{\textwidth}{!}{\input{figures/charge.pgf}}
    \caption{\label{fig:output} Output $\bm{\hat{t}}, \bm{\hat{q}}$}
  \end{subfigure}
  \caption{\label{fig:io}Waveform analysis takes PMT waveform as input~\subref{fig:input} and outputs the inferred timings $t_i$ and charges $q_i$ of the PEs~\subref{fig:output}.}
\end{figure}

$\sigma_q$, the standard deviation of $q_i$, is $\num{\sim 0.4}$.  Such a spread of $q_i$ makes $\hat{t}_i$ ambiguous. For example, $\hat{q}_i=1.6$ can be generated by 1 or 2~PEs, or at a lower probability, even 3~PEs at $t_i$. 1~PE of $q_i=1$ may be inferred as $\hat{t}_i$ and $\hat{t}_{i+1}$ with $\hat{q}_i=\hat{q}_{i+1}=0.5$ and mis-interpreted as 2~PEs.  Consequently, $\hat{N}$ fails to give an estimation of $N_\mathrm{PE}$.  This is the fundamental difficulty in using $\bm{\hat{t}}$ and $\bm{\hat{q}}$ consistently for timing measurement. 

\subsection{Evaluation criteria}
\label{sec:criteria}
Facing the ambiguity of $t_i$ and $q_i$, a set of evaluation criteria is developed to assess algorithms' performance. 

\subsubsection{Kullback-Leibler divergence}
\label{sec:pseudo}

One idea to plug $\hat{t}_i$ into \eqref{eq:2} is to interpret $\hat{q}_i$ as weights,
\begin{equation}
  \label{eq:pseudo}
  \hat{t}_\mathrm{KL} = \arg\underset{t_0}{\max} \prod_{i=1}^{\hat{N}} \left[\phi(\hat{t}_i-t_0)\right]^{\hat{q}_i}.
\end{equation}
$\hat{t}_\mathrm{KL}$ is not a valid, but a pseudo-likelihood estimator. However, it can be understood as the Kullback-Leibler divergence~\cite{kullback_information_1951} between an inferred and the true light curves.

$\bm{\hat{t}}$ and $\bm{\hat{q}}$ give a light curve estimator $\hat{\phi}(t)$,
\begin{equation}
  \label{eq:lc}
  \hat{\phi}(t) = \sum_{i=1}^{\hat{N}} \hat{q}_i\delta(t-\hat{t}_i)
\end{equation}
up to a normalization factor, where $\delta(t)$ is the Dirac delta function to model discrete values.  Consider the non-normalized KL divergence~\cite{mihoko_robust_2002} from $\hat{\phi}(t)$ to $\phi(t-t_{0})$,
\begin{equation}
  \begin{aligned}
    D_\mathrm{KL}\left[\hat{\phi}(t) \parallel \phi(t-t_0)\right] & =\int \left[\hat{\phi}(t) \log\frac{\hat{\phi}(t)}{\phi(t-t_0)} + \phi(t-t_0) - \hat{\phi}(t) \right]\mathrm{d}t \\
    & = - \int \hat{\phi}(t) \log\phi(t-t_0)\mathrm{d}t + \int \left[\hat{\phi}(t) \log\hat{\phi}(t) + \phi(t) - \hat{\phi}(t) \right]\mathrm{d}t \\
    & = - \sum_{i=1}^{\hat{N}}\int \hat{q}_i\delta(t-\hat{t_i}) \log\phi(t-t_0)\mathrm{d}t + C \\
    & = -\log \prod_{i=1}^{\hat{N}} \left[\phi(\hat{t}_i-t_0)\right]^{\hat{q}_i} + C
  \label{eq:kl}
  \end{aligned}
\end{equation}
where $C$ is a constant in regard to $t_0$.  Comparing with \eqref{eq:pseudo}, it is obvious that
\begin{equation}
  \label{eq:kl2}
  \hat{t}_\mathrm{KL} = \arg\underset{t_0}{\min}~D_\mathrm{KL}\left[\hat{\phi}(t) \parallel \phi(t-t_0)\right].
\end{equation}
Similar to $\sigma_\mathrm{ALL}$, we use the standard deviation of $\hat{t}_\mathrm{KL} - t_0$ for a 
batch of waveforms, denoted as $\sigma_\mathrm{KL}$, to measure the resolutions of waveform analysis methods.

\subsubsection{Residual sum of squares}
\label{sec:rss}

Following~\eqref{eq:1} and~\eqref{eq:lc}, construct an estimator of waveform,
\begin{equation}
  \label{eq:w-hat}
  \hat{w}(t) = \sum_{i=1}^{\hat{N}}\hat{q}_i V_\mathrm{PE}(t-\hat{t}_i) = \hat{\phi}(t) \otimes V_\mathrm{PE}(t).
\end{equation}
Sum of squares of the residual $\hat{w}(t) - w(t)$ is a natural $\ell_2$-metric inspired by the Gaussian noise term in \eqref{eq:1},
\begin{equation}
  \label{eq:rss}
  \mathrm{RSS}_w\coloneqq\int\left[\hat{w}(t) - w(t)\right]^2\mathrm{d}t.
\end{equation}
RSS cannot compare a discrete function with a continuous one.  Even with two similar functions, RSS is not sensitive to timing.  In figure~\ref{fig:l2}, RSS of  $a-b_{1}$ and $a-b_{2}$ are both 0.25.  Intuitively $b_1$ is closer to $a$ than $b_2$ and RSS fails to capture such a difference.
\begin{figure}[H]
  \centering
  \resizebox{\textwidth}{!}{\input{figures/tab.pgf}}
  \caption{\label{fig:l2} $b_1$ and $b_2$ have the same $\mathrm{RSS}=0.25$ to $a$, but $b_1$ is closer in timing to $a$.}
\end{figure}

The PE space is sparse. RSS metric of $\tilde{\phi}(t)$ and $\hat{\phi}(t)$ suffers from the ambiguity in figure~\ref{fig:l2}: functions having different timing properties are indistinguishable by $\tilde{\phi}(t)$-$\hat{\phi}(t)$ RSS.  Unless specified otherwise, $\mathrm{RSS}$ refers to $\mathrm{RSS}_w$ in~\eqref{eq:rss}.

\subsubsection{Wasserstein distance}
\label{sec:W-dist}

\input{fom}

Waveform analysis algorithms are investigated in categories of heuristics, deconvolution, neuron network and regression by the discussed evaluation criteria.

\subsection{Heuristic methods}
By directly extracting waveform features, heuristic methods are straightforward to implement and thus widely used. 

\subsubsection{Waveform shifting}
\label{sec:shifting}
A threshold $V_\mathrm{th}$ is selected with respect to electronics noise in the waveforms. Waveform entries $w(t_i)$ exceeding $V_\mathrm{th}$ are selected and shifted by $\Delta t$ according to the shape of SPE response $V_\mathrm{PE}(t)$ in \eqref{eq:dayaspe}. $\hat{q}_i$ are normalized to total charge by integration of the waveform $w(t_i)$.  Waveform shifting does minimal analysis and serves as a baseline method.  The demonstration in figure~\ref{fig:shifting} gives $D_w = \SI{8.72}{ns}$.

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/threshold.pgf}}
    \caption{\label{fig:shifting} $\mathrm{RSS}=\SI{4805.4}{mV^2},D_w=\SI{8.72}{ns},\Delta t_0=\SI{10.74}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/findpeak.pgf}}
    \caption{\label{fig:peak} $\mathrm{RSS}=\SI{317.3}{mV^2},D_w=\SI{2.40}{ns},\Delta t_0=\SI{-1.93}{ns}$}
  \end{subfigure}
  \caption{Heuristic methods demonstrations when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. Waveform shifting is in~\subref{fig:shifting}. Peak finding is in~\subref{fig:peak}.}
\end{figure}

\subsubsection{Peak finding}
\label{sec:findpeak}

In the peak finding method, local maxima are extracted in Savitzky-Golay filter~\cite{savitzky_smoothing_1964} smoothed waveforms.  A minus shift $\Delta t$ is applied to peak positions $\hat{t}_i$ to minimize $D_w$ considering SPE response $V_\mathrm{PE}(t)$ in \eqref{eq:dayaspe}. The reconstructed charges $\hat{q}_i$ of $\hat{t}_i$ are proportional to the peak voltages, which are normalized to the total charge by integration of the waveform.  This works well for lower PE counts. When PE's pile-up closely, peaks overlap and cannot be extracted reliably. A demonstration shown in figure~\ref{fig:peak} has $D_w = \SI{2.40}{ns}$. 

\subsection{Deconvolution}
\label{sec:deconv}
While heuristic methods extract only a timing shift, deconvolution methods exploit the full SPE shape.  Deconvolution can accommodate overshoots and is selected by JUNO prototype to measure charges~\cite{zhang_comparison_2019}.  Smoothing is necessary because deconvolution does not model the white noise. 

\subsubsection{Fourier deconvolution}

Take Fourier transform $\mathcal{F}$ of \eqref{eq:1},
\begin{equation}
  \label{eq:fourier}
  \begin{aligned}
  \mathcal{F}[w] & = \mathcal{F}[\tilde{\phi}]\mathcal{F}[V_\mathrm{PE}] + \mathcal{F}[\epsilon]\\
  \implies \mathcal{F}[\tilde{\phi}] & = \frac{\mathcal{F}[w]}{\mathcal{F}[V_\mathrm{PE}]} - \frac{\mathcal{F}[\epsilon]}{\mathcal{F}[V_\mathrm{PE}]}.
  \end{aligned}
\end{equation}
The noise term $\epsilon$ is unknown, but can be suppressed by a threshold $V_\mathrm{th}$ and a low pass filter if $V_\mathrm{PE}(t)$ is smooth.  The filter can be realized by a low pass filter multiplier $R$ in the frequency domain. The estimator $\hat{\phi}(t)$ takes the over-threshold part of inverse Fourier transformationed function of $t$ and is defined in 3 steps,
\begin{equation}
  \label{eq:fdconv2}
  \begin{aligned}
    \hat{\phi}''(t) & = \mathcal{F}^{-1}\left[\frac{R \mathcal{F}[w]}{\mathcal{F}[V_\mathrm{PE}]}\right](t) \\
    \hat{\phi}'(t) & = \hat{\phi}''(t) I(\hat{\phi}''(t) - V_\mathrm{th}) \\
    \hat{\phi}(t) & = \hat{\alpha}\hat{\phi}'(t) \\
  \end{aligned}
\end{equation}
where $I(x)$ is the indicator function. Instead of using the total relative charge, $\hat{\alpha}$ is derived with a minimization procedure, 
\begin{equation}
  \begin{aligned}
  \label{eq:id}
  I(x) = \left\{
    \begin{array}{ll}
      1 & \mbox{, if $x\ge0$}, \\
      0 & \mbox{, otherwise}
    \end{array}
    \right.
    \quad
    \hat{\alpha} = \arg \underset{\alpha'}{\min}\mathrm{RSS}\left[\alpha'\hat{\phi}'(t)\otimes V_\mathrm{PE}(t),w(t)\right]. \\
  \end{aligned}
\end{equation}
This $\hat{\alpha}$ scaling process provides the capability to eliminate the influence of noise. 

Figure~\ref{fig:fd} illustrates the algorithm with an example having $D_w = \SI{2.03}{ns}$. 

\begin{figure}[H]
  \begin{subfigure}{0.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/fftrans.pgf}}
    \caption{\label{fig:fd} $\mathrm{RSS}=\SI{124.7}{mV^2},D_w=\SI{2.03}{ns},\Delta t_0=\SI{-1.16}{ns}$}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/lucyddm.pgf}}
    \caption{\label{fig:lucy} $\mathrm{RSS}=\SI{70.3}{mV^2},D_w=\SI{1.10}{ns},\Delta t_0=\SI{-0.75}{ns}$}
  \end{subfigure}
  \caption{Deconvolution demonstrations when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. Fourier deconvolution is in~\subref{fig:fd}. Lucy direct demodulation is in~\subref{fig:lucy}.}
\end{figure}

\subsubsection{Lucy direct demodulation}

Lucy-Richardson direct demodulation (LucyDDM)~\cite{lucy_iterative_1974} uses non-linear iteration to calculate deconvolution and is used in astronomy \cite{li_richardson-lucy_2019}, image processing, and many other fields.  Viewing $V_{\mathrm{PE}*}(t-s)$ as a conditional probability distribution $P(t|s)$ of PMT amplified electron time $t$ given PE occurring at $s$, by the Bayesian rule,
\begin{equation}
  \label{eq:lucy}
  \tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s) = \tilde{\phi}_*(s)P(t|s) = P(t,s) = \tilde{w}_*(t)P(s|t)
\end{equation}
where $P(t,s)$ is the joint distribution of amplified electron and PE timings.  Therefore,
\begin{equation}
  \label{eq:ptt}
  P(s|t) = \frac{\tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s)}{\tilde{w}_*(t)} = \frac{\tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s)}{\int\tilde{\phi}_*(s') V_{\mathrm{PE}*}(t-s')\mathrm{d}s'}.
\end{equation}
Construct $\phi_*$ recurrence relation by replacing $\tilde{w}_*$ with the observed normalized waveform $w_*$, where negative parts are set to 0,
\begin{equation}
  \label{eq:iter}
  \begin{aligned}
    \tilde{\phi}_*(s) & = \int P(s|t) w_*(t)\mathrm{d}t =  \int \frac{\tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s)}{\int\tilde{\phi}_*(s') V_{\mathrm{PE}*}(t-s')\mathrm{d}s'} w_*(t) \mathrm{d}t \\
    \implies \hat{\phi}_*^{n+1}(s) & = \int \frac{\hat{\phi}_*^n(s) V_{\mathrm{PE}*}(t-s)}{\int\hat{\phi}_*^n(s') V_{\mathrm{PE}*}(t-s')\mathrm{d}s'} w_*(t) \mathrm{d}t.
  \end{aligned}
\end{equation}
Like Fourier deconvolution in \eqref{eq:fdconv2}, this method thresholds and scales the converged $\hat{\phi}_*^{n}$. LucyDDM constructs $\hat{\phi}$ to be positive, making it more resilient to noise.  Figure~\ref{fig:lucy} shows a LucyDDM example with $D_w = \SI{1.10}{ns}$.

\subsection{Convolutional neural network}

% Network structure
Advances in neural networks have brought breakthroughs in various domains like computer vision~\cite{he_deep_2015} and natural language processing~\cite{vaswani_attention_2017}. As an efficient composition of weighted additions and pointwise non-linear functions, neural networks have prevailed against many traditional algorithms. We introduce a multi-layered convolutional neural network~(CNN) to process time-sensitive PEs $\tilde{\phi}(t)$ from waveforms $w(t)$ in \eqref{eq:1}.  Because $V_\mathrm{PE}(t)$ is localized, the convolutional widths are selected accordingly.

Although advanced structures like residual connection and strategies like batch-norm alleviated training convergence problems for deep networks, the pulse-shape, and universality of $V_\mathrm{PE}(t)$ for all the PE signals allow patterns to be recognized in only a few layers.  We chose a shadow network structure of 5 layers~(figure~\ref{fig:struct}) to cut the computation in massive experimental data.  Deeper structures give diminishing returns.

The workflow of data processing consists of training and predicting. Training is to find an efficient mapping from detector waveforms $w(t)$ to PE $\tilde{\phi}(t)$ with backpropagation methods by supervised learning.  The learning is achieved by minimizing the Wasserstein distance loss $W_d$ between the truth $\tilde{\phi}$ and predicted $\hat{\phi}(t)$.  As discussed in section~\ref{sec:W-dist}, $W_d$ is chosen to handle the PE sparsity and output proximity judgment in training iterations.  KL divergences discussed in section~\ref{sec:pseudo} is a candidate, but it is still to be explored to implement in a trainable form for backpropagation.  RSS is less timing sensitive.  The evolution of Wasserstein distance during training is shown in figure~\ref{fig:loss}.

\begin{figure}[H]
  \begin{subfigure}{0.35\textwidth}
    \centering
    \begin{adjustbox}{width=0.65\textwidth}
      \input{model}
    \end{adjustbox}
    \caption{\label{fig:struct} CNN structure}
  \end{subfigure}
  \begin{subfigure}{0.6\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/epoch.pgf}}
    \caption{\label{fig:loss} Evolution of loss}
  \end{subfigure}
  \caption{\label{fig:CNN} Training process of a CNN. A shadow network structure of 5 layers is in~\subref{fig:struct} and the evolution of Wasserstein distance during training is in~\subref{fig:loss}.}
\end{figure}

The product of network training is a function that creates PE outputs $\hat{\phi}(t)$ from waveform inputs $w(t)$.  Since $D_w$ disregards normalization, $\hat{\phi}$ is scaled to $\hat{\alpha}\hat{\phi}$ following~\eqref{eq:fdconv2}. A demonstration shown in figure~\ref{fig:cnn} has $D_w = \SI{0.75}{ns}$. 

The result of $\hat{\phi}(t)$ when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:cnn-npe}. The left panel is the boxplot which shows the $D_d$ percentile vs $N_{pe}$, and the right panel is the overall histogram of $D_d$. 

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/takara.pgf}}
    \caption{\label{fig:cnn}$\mathrm{RSS}=\SI{10.0}{mV^2},D_w=\SI{0.64}{ns},\Delta t_0=\SI{-3.05}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/takarachargestats.pgf}}
    \caption{\label{fig:cnn-npe} $D_d$ vs $N_{pe}$ Histogram \& Boxplot}
  \end{subfigure}
  \caption{CNN results when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. A demonstration shows in~\subref{fig:cnn}.}
\end{figure}

\subsection{Regression analysis}
With waveform generative model in~\eqref{eq:1}, regression analysis is the best for waveform analysis.  Although computational complexity hindered regression from being used in vast volumes of raw data, the advancement of big data infrastructure and sparsity research makes the advantage of regression more pronounced.

In (\ref{eq:w-hat}), replace $\hat{N}$ with a fixed sample size $N_s$ and $\hat{t}_i$ with a fixed vector of timings $t'_i$,
\begin{equation}
  \label{eq:gd}
  w'(t) = \sum_{i=1}^{N_s}q'_iV_\mathrm{PE}(t-t'_i).
\end{equation}

When $\{t'_i\}$ is dense enough, $\{\hat{q}_i\}$ determines the inferred PE distribution $\hat{\phi}(t)$,
\begin{equation}
  \label{eq:gd-phi}
  \hat{\phi}(t) = \sum_{i=1}^{N_s}\hat{q}_i\delta(t-t'_i).
\end{equation}

It is tempting to replace the dense $\bm{t'}$ grid in \eqref{eq:gd} with a length-varying vector of sparse PEs. However, because the truth $N_\mathrm{PE}$ is unknown, that formulates an explicit trans-dimensional model.  Although worth trying, model selection will complicate the optimization and is beyond the scope of this study.

Because the output $\hat{\phi}(t)$ from a deconvolution method~(section~\ref{sec:deconv}) covers locations of non-zero $\bm{q}'$, its discretization can initialize the $\bm{t}'$ grid to reduce $N_s$ and consequently the time consumption.

\subsubsection{Gradient descent}

Construct estimators $\hat{q}_i$ by minimizing RSS of $w'(t)$ and $w(t)$,
\begin{equation}
  \label{eq:gd-q}
  \bm{\hat{q}} = \arg \underset{\bm{q'}}{\min} \mathrm{RSS}\left[w'(t),w(t)\right].
\end{equation}

The optimizer is realized with limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm with bound constraint~\cite{byrd_limited_1995}.  It can impose constraints of $q'_i > 0$. Also like Fourier deconvolution in \eqref{eq:fdconv2}, this method thresholds and scales the converged $\bm{\hat{q}}$. The demonstration~\ref{fig:fitting} here shows a result with $D_w = \SI{0.75}{ns}$.

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/xiaopeip.pgf}}
    \caption{\label{fig:fitting}$\mathrm{RSS}=\SI{13.0}{mV^2},D_w=\SI{0.75}{ns},\Delta{t_0}=\SI{-2.27}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/xiaopeipchargestats.pgf}}
    \caption{\label{fig:fitting-npe} $D_d$ vs $N_{pe}$ Boxplot \& Histogram}
  \end{subfigure}
  \caption{Gradient descent results when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. A demonstration shows in~\subref{fig:fitting}.}
\end{figure}

The result of $\hat{q}$ when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:fitting-npe}. The left panel is the boxplot which shows the $D_d$ percentile vs $N_{pe}$, and the right panel is the overall histogram of $D_d$. 

\subsubsection{Markov chain Monte Carlo}
\label{subsec:mcmc}
The optimization problem in \eqref{eq:gd-q} is in general not convex and may be stuck in local minima.  Markov Chain Monte Carlo~(MCMC) has the potential to overcome this difficulty.

Hamiltonian Monte Carlo~(HMC) is an efficient MCMC for high dimensional distributions~\cite{neal_mcmc_2012} such as the posterior of $\bm{q'}$.  The posterior is combined with a $\bm{q'}$ prior conditioned on $t_0$ to form a hierarchical model,
\begin{equation}
  \begin{aligned}
    t_{0} &\sim \mathrm{Uniform}(0, \overline{t_0}) \\
    p_i &= \mu \int_{t'_i-\frac{\Delta t'}{2}}^{t'_i+\frac{\Delta t'}{2}} \phi(t' - t_0)\mathrm{d}t' \approx \mu\phi(t'_i - t_0)\Delta{t'}  \\
    z_i &\sim \mathrm{Binomial}(p_i) \\
    q'_{i,0}&=0\\
    q'_{i,1}& \sim \mathrm{Normal}(1, \sigma_q)\\
    q'_i &= q'_{i,z_i}\\
    w'(t) & = \sum_{i=1}^{N_s}q'_iV_\mathrm{PE}(t-t'_i)\\
    w(t) &\sim \mathrm{Normal}(w'(t), \sigma_\epsilon)
  \end{aligned}
  \label{eq:mixnormal}
\end{equation}
where $t_{0}$ is sampled from a uniform prior with upper bound $\overline{t_{0}}$, and $p_i$ the expection of a PE hitting $(t_{i} - \frac{\Delta t'}{2}, t_{i} + \frac{\Delta t'}{2})$ when $\Delta t'$ is small enough. $q'_i$ is a mixture of 0 (no PE) and normally distributed $q'_{i,1}$ (1 PE).  The inferred waveform $w'(t)$ differs from observation $w(t)$ by a white noise $\epsilon(t) \sim \mathrm{Normal}(0, \sigma_\epsilon)$.  When $\Delta{t'} \to 0$, formulation~\eqref{eq:mixnormal} is equivalent to~\eqref{eq:1} up to a shift by $t_0$.  Instead of using $N_\mathrm{PE}$ to govern the number of pairs $(t_i, q_i)$, the trans-dimensionality is encoded implicitly by $z_i$ to turn on or off $q'_i$.

Posterior samples of $t_0$ and $\bm{q'}$ are generated by HMC.   Ergodicity of HMC prevents local maxima from trapping the sampler.  We use NumPyro~\cite{phan2019composable}, a graceful probabilistic programming library, to drive the HMC.  Instead of fixing to 0, $q'_{i,0}$ is modeled as a narrow $\mathrm{Normal}(0, \sigma_0)$ to adapt to NumPyro.

$\hat{t}_0$ and $\hat{q}_i$ are constructed as the mean estimators of posterior samples.  Unlike the pseudo-likelihood $\hat{t}_\mathrm{KL}$ discussed in section~\ref{sec:pseudo}, $\hat{t}_0$ is a genuine Bayesian estimator.  Construct $\hat{\phi}(t)$ as,
\begin{equation}
  \label{eq:mcmc-phi}
  \hat{\phi}(t) = \sum_{i=1}^{N_s}\hat{q}_i\delta(t-t'_i).
\end{equation}

$D_w$ when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:mcmc}.
\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/mcmc.pgf}}
    \caption{\label{fig:mcmc}$\mathrm{RSS}=\SI{16.25}{mV^2},D_w=\SI{0.76}{ns},\Delta{t_0}=\SI{-2.48}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/mcmcchargestats.pgf}}
    \caption{\label{fig:mcmc-npe} $D_d$ vs $N_{pe}$ Boxplot \& Histogram}
  \end{subfigure}
  \caption{MCMC results results when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. A demonstration shows in~\subref{fig:mcmc}.}
\end{figure}

\subsubsection{Fast Bayesian matching pursuit}
\label{subsec:fbmp}
$w(t)$ is a Gaussian process subject to $\bm{z}$,
\begin{equation}
    w(t)|\bm{z} \sim \mathcal{GP}\left[\sum_{i|z_i=1}V_\mathrm{PE}(t-t'_i), \sigma_q^2 \sum_{i|z_i=1}V_\mathrm{PE}(t'_i-t)V_\mathrm{PE}(s-t'_i) + \sigma_\epsilon^2 \right],
\end{equation}
where $\bm{z}$ controls which $t'_i$ are used.  In the numerical calculation, the Gaussian process is discretized as a multivariate normal distribution together with $\bm{q}'$
\begin{equation}
\label{eq:mgauss}
\begin{aligned}
    \left.
    \begin{bmatrix}
        \bm{w} \\
        \bm{q}'
    \end{bmatrix}
    \right\vert\bm{z}
    &\sim \mathrm{Normal}\left(
    \begin{bmatrix}
        \bm{V}_\mathrm{PE}\bm{z} \\
        \bm{z}
    \end{bmatrix}, 
    \begin{bmatrix}
        \bm{\Sigma}_z & \bm{V}_\mathrm{PE}\bm{Z} \\
        \bm{Z}\bm{V}_\mathrm{PE}^\intercal & \bm{Z}
    \end{bmatrix}
    \right) \\
    \bm{\Sigma}_z &= \bm{V}_\mathrm{PE}\bm{Z}\bm{V}_\mathrm{PE}^\intercal+\sigma_\epsilon^2\bm{I}
\end{aligned}
\end{equation}
where $\bm{Z}$ is the diagonal matrix of vector $\bm{z}$ controlling $q'_i$. $\bm{I}$ is a unit matrix. Here $\bm{z}$ is not the same as it in~\ref{subsec:mcmc}: 

\begin{equation}
  \begin{aligned}
    p_i &= \mu \int_{t'_i-\frac{\Delta t'}{2}}^{t'_i+\frac{\Delta t'}{2}} \phi(t' - t_0)\mathrm{d}t' \\
    z_i &\sim \mathrm{Poisson}(p_i).
  \end{aligned}
  \label{eq:poissonfbmp}
\end{equation}

Denote a set $\mathcal{Z}$ to contain infinite combinations of $\bm{z}$. Only few $\bm{z}$'s gives finite probabilities of $\bm{w}$, leading to Fast Bayesian Matching Pursuit~(FBMP), a parameter estimation method for sparse linear models~\cite{schniter_fast_nodate}.  FBMP defines a targeted subset $\mathcal{Z}'$ of $\mathcal{Z}$ with only the $\bm{z}$ giving non-negligible $p(\bm{w}|\bm{z})$.  The posterior of $\bm{z}$ is approximated as
\begin{equation}
    \label{eq:z}
    \begin{aligned}
    p(\bm{z}|\bm{w}) &= \frac{p(\bm{w}|\bm{z})p(\bm{z})}{\sum_{\bm{z}'\in\mathcal{Z}}p(\bm{w}|\bm{z'})p(\bm{z'})} \\
    &\approx \frac{p(\bm{w}|\bm{z})p(\bm{z})}{\sum_{\bm{z}'\in\mathcal{Z}'}p(\bm{w}|\bm{z'})p(\bm{z'})}.
    \end{aligned}
\end{equation}
The prior $p(\bm{z})$ is given by the Poisson distributions of binned light curve,
\begin{equation}
\begin{aligned}
    p(\bm{z}|t_0, \mu) &= \prod_{i}\mathrm{Poisson}(z_i,p_i)
\end{aligned}
\end{equation}
where $\mu$ is the prior expected number of PEs and $\sum_i p_i = \mu$ by the normalization condition. 

The nominator logarithm of \eqref{eq:z},
\begin{equation}
    \label{eq:metric}
    \begin{aligned}
        \nu =& \log[p(\bm{w},\bm{z})] = \log[p(\bm{w}|\bm{z})p(\bm{z})] \\
        =& -\frac{1}{2}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})^\intercal\bm{\Sigma}_z^{-1}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})-\frac{1}{2}\log\det\bm{\Sigma}_z-\frac{N_s}{2}\log2\pi \\
        & +\sum_{i}\log{\mathrm{Poisson}(z_i,p_i)}
    \end{aligned}
\end{equation}
is treated as a \textit{model selection metric}.  A repeated greedy search (RGS) is performed to construct the target set $\mathcal{Z}'$ guided by \eqref{eq:metric}. Additionally, a linear algebraic iteration reduces the complexity of $p(\bm{z}|\bm{w})$ calculation~\cite{schniter_fast_nodate}. 

For each $\bm{z} \in \mathcal{Z}'$, by \eqref{eq:mgauss} the expectation estimation of $\bm{q}'$ is, 
\begin{align}
    \hat{\bm{q}}_z = E(\bm{q}'|\bm{w},\bm{z}) &= \bm{z} + \bm{Z}\bm{V}_\mathrm{PE}^\intercal\bm{\Sigma}_z^{-1}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})
    \label{eq:fbmpcharge}
\end{align}

Finally, all the $\bm{z} \in \mathcal{Z}'$ and their corresponding posterior probabilities $p(\bm{z}|\bm{w})$, and estimations of $\hat{\bm{q}}_z$ are collected to be the total posterior distribution. $\hat{\bm{z}}$ is the PE number in each time bin $\hat{\bm{t}}$ unartificially. $\hat{\bm{q}}$, which is the total charge in each time bin is estimated by the maximum posterior probability,
\begin{equation}
    \begin{aligned}
        \label{eq:zposterior}
        \hat{\bm{z}} &= \arg \underset{\bm{z}}{\max} p(\bm{z}|\bm{w}) \\
        \hat{\bm{q}} &= \hat{\bm{q}}_{\hat{z}} \\
        \hat{w}(t) &= \sum_{i} \hat{q}_i V_\mathrm{PE}(t-t'_i)
    \end{aligned}
\end{equation}

Figure~\ref{fig:fbmp} shows one of the FBMP results. We can see the original waveform $w(t)$ and reconstructed waveform $\hat{w}(t)$ are very similar. The reconstructed and truth $\bm{q}$ are also similar.

\begin{figure}[H]
    \centering
    \resizebox{0.6\textwidth}{!}{\input{figures/demoe2c0.pgf}}
    \caption{\label{fig:fbmp} FBMP Demo, $\mathrm{RSS}=\SI{15.8}{mV^2},D_w=\SI{0.59}{ns},\Delta{t_0}=\SI{-3.51}{ns}$}
\end{figure}

The result of $\hat{q}$ when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:fbmp-npe}. The left panel is the boxplot which shows the $D_w$ percentiles vs $N_{pe}$, and the right panel is the overall histogram of $D_w$. 

\begin{figure}[H]
    \centering
    \resizebox{0.5\textwidth}{!}{\input{figures/fbmpchargestats.pgf}}
    \caption{\label{fig:fbmp-npe} $D_w$ vs $N_{pe}$ Boxplot \& Histogram, $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$, method is FBMP}
\end{figure}

The denominator of \eqref{eq:z} is actually the evidence $p(\bm{w}|t_0, \mu)$, which provides a superior estimator of $\mu$ and $t_0$ to \eqref{eq:pseudo},
\begin{equation}
\begin{aligned}
    \label{eq:bayesianinter}
    \left(\hat{t}_0, \hat{\mu}\right) &= \arg\underset{t_0,\mu}{\max} p(\bm{w} | t_0, \mu) \\
    p(\bm{w}|t_0, \mu) &= \sum_{\bm{z}'\in\mathcal{Z}'}p(\bm{w}|\bm{z}',t_0,\mu)p(\bm{z}'|t_0,\mu) \\
    &= \sum_{\bm{z}'\in\mathcal{Z}'}p(\bm{w}|\bm{z}')p(\bm{z}'|t_0,\mu).
\end{aligned}
\end{equation}
where $p(\bm{w}|z', t_0, \mu) = p(\bm{w}|')$ because $\bm{w}$ does not depend on $t_0$ or $\mu$ if $z'$ is given.  If $t_0$ and $\mu$ are predicted by a event model $p(t_0, \mu | \mathcal{E})$, FBMP is extended to event reconstruction bypassing the need to save PE $\bm{t}'$ and $\bm{q}'$.  While FBMP provide the Bayesian interface of waveform analysis and event reconstruction, it is however out of the scope of this article and will be covered elsewhere.

\subsection{Performance}

% Methods

The $D_w$ summary of all methods is plotted in figure~\ref{fig:chargesummary}, with all dataset parameters $\tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. The errorbar shows 5 to 95 percentile of Wasserstein distance distribution. 

\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{\input{figures/summarycharge.pgf}}
    \caption{\label{fig:chargesummary} $W_{d}$ of methods with light curve $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$}
\end{figure}

It is apparent that FindPeak, Shift, and FFT are second-rated among all 8 methods, and on time resolution and charge reconstruction which we will discuss in~\ref{subsec:timeresolution} and~\ref{subsec:chargereconstruction}, MCMC does not perform well and is time-consuming. So we will only discuss the other 4 methods below. 

% Figure of Merits

The $D_w$ summary of 4 methods (LucyDDM, Fitting, CNN, FBMP) is plotted in~\ref{fig:wdistsummary}, with all dataset parameters combination. The errorbar shows 5 to 95 percentile of Wasserstein distance distribution. Wasserstein distances of all 4 methods are approaching each other when $\mu$ increases. 

We can see that as method CNN uses Wasserstein distance as loss directly during training, its Wasserstein distance is the smallest. While Fitting treats RSS as its loss, FBMP obtains the least RSS among 4 methods (see figure~\ref{fig:rsssummary}) and CNN follows. 
\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{\input{figures/vs-wdist.pgf}}
    \caption{\label{fig:wdistsummary} $D_w$ of methods, error bar 5--95 percentile ($\hat{q}$)}
\end{figure}

\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{\input{figures/vs-rss.pgf}}
    \caption{\label{fig:rsssummary} RSS of methods, error bar 5--95 percentile ($\hat{q}$)}
\end{figure}
