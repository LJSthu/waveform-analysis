\section{Algorithms and Performance} % (fold)
\label{sec:algorithm}

The goal of waveform analysis is to construct estimators $\hat{t}_i$ and $\hat{q}_i$ from waveform $w(t)$, in which the output indices $i$ is from 1 to $\hat{N}$.  Figure~\ref{fig:io} illustrates the input $w(t)$ and the output $\{(\hat{t}_i, \hat{q}_i)|i=1,\ldots,\hat{N}\}$.
\begin{figure}[H]
  \centering
  \begin{subfigure}{.45\textwidth}
    \resizebox{\textwidth}{!}{\input{figures/wave.pgf}}
    \caption{\label{fig:input} Input $w(t)$}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
    \resizebox{\textwidth}{!}{\input{figures/charge.pgf}}
    \caption{\label{fig:output} Output $\{(\hat{t}_i, \hat{q}_i)|i=1,\ldots,\hat{N}\}$}
  \end{subfigure}
  \caption{\label{fig:io}Waveform analysis takes PMT waveform as input~\subref{fig:input} and outputs the inferred timings $t_i$ and charges $q_i$ of the PEs~\subref{fig:output}.}
\end{figure}

$\sigma_q$, the standard deviation of $q_i$, is $\num{\sim 0.4}$.  Such a spread of $q_i$ makes $\hat{t}_i$ ambiguous. For example, $\hat{q}_i=0.4$ can be generated by 0 or 1~PEs, or at a lower probability, even 2~PEs at $t_i$. 1~PE of $q_i=1$ may be inferred as $\hat{t}_i$ and $\hat{t}_{i+1}$ with $\hat{q}_i=\hat{q}_{i+1}=0.5$ and mis-interpreted as 2~PEs.  Consequently, $\hat{N}$ fails to give an estimation of $N_\mathrm{PE}$.  This is the fundamental difficulty in using $\{(\hat{t}_i, \hat{q}_i)|i=1,\ldots,\hat{N}\}$ consistently for timing measurement.

\subsection{Evaluation Criteria}
\label{sec:criteria}
Facing the ambiguous of $t_i$ and $q_i$, a set of evaluation criteria is developed to assess the performance of algorithms. 

\subsubsection{Kullback-Leibler Divergence}
\label{sec:pseudo}

One idea to plug $\hat{t}_i$ into \eqref{eq:2} is to interpret $\hat{q}_i$ as weights,
\begin{equation}
  \label{eq:pseudo}
  \hat{t}_\mathrm{KL} = \arg\underset{t_0}{\max} \prod_{i=1}^{\hat{N}} \left[\phi(\hat{t}_i-t_0)\right]^{\hat{q}_i}.
\end{equation}
$\hat{t}_\mathrm{KL}$ is not a valid, but a pseudo-likelihood estimator. However, it can be understood as the Kullback-Leibler divergence~\cite{kullback_information_1951} between an inferred and the true light curves.

$\{(\hat{t}_i, \hat{q}_i)|i=1,\ldots,\hat{N}\}$ gives a light curve estimator $\hat{\phi}(t)$,
\begin{equation}
  \label{eq:lc}
  \hat{\phi}(t) = \sum_{i=1}^{\hat{N}} \hat{q}_i\delta(t-\hat{t_i})
\end{equation}
up to a normalization factor, where $\delta(t)$ is the Dirac delta function to model discrete values.  Consider the non-normalized KL divergence from $\hat{\phi}(t)$ to $\phi(t)$,
\begin{equation}
  \begin{aligned}
    D_\mathrm{KL}\left[\hat{\phi}(t) \parallel \phi(t-t_0)\right] & =\int \left[\hat{\phi}(t) \log\frac{\hat{\phi}(t)}{\phi(t-t_0)} + \phi(t-t_0) - \hat{\phi}(t) \right]\mathrm{d}t \\
    & = - \int \hat{\phi}(t) \log\phi(t-t_0)\mathrm{d}t + \int \left[\hat{\phi}(t) \log\hat{\phi}(t) + \phi(t) - \hat{\phi}(t) \right]\mathrm{d}t \\
    & = - \sum_{i=1}^{\hat{N}}\int \hat{q}_i\delta(t-\hat{t_i}) \log\phi(t-t_0)\mathrm{d}t + C \\
    & = -\log \prod_{i=1}^{\hat{N}} \left[\phi(\hat{t}_i-t_0)\right]^{\hat{q}_i} + C
  \label{eq:kl}
  \end{aligned}
\end{equation}
where $C$ is a constant in regard to $t_0$.  Comparing with \eqref{eq:pseudo}, it is obvious that
\begin{equation}
  \label{eq:kl2}
  \hat{t}_\mathrm{KL} = \arg\underset{t_0}{\min}~D_\mathrm{KL}\left[\hat{\phi}(t) \parallel \phi(t-t_0)\right].
\end{equation}
Similar to $\sigma_\mathrm{ALL}$, we use the standard deviation of $\hat{t}_\mathrm{KL}$ denoted as $\sigma_\mathrm{KL}$ to measure the resolutions of waveform analysis methods.

\subsubsection{Residual Sum of Squares}
\label{sec:rss}

Following~\eqref{eq:1} and~\eqref{eq:lc}, construct an estimator of waveform,
\begin{equation}
  \label{eq:w-hat}
  \hat{w}(t) = \sum_{i=1}^{\hat{N}}\hat{q}_iV_\mathrm{PE}(t-\hat{t}_i) = \hat{\phi}(t) \otimes V_\mathrm{PE}(t).
\end{equation}
Sum of squares of the residual $\hat{w}(t) - w(t)$ is a natural $\ell_2$-metric inspired by the Gaussian noise term in \eqref{eq:1},
\begin{equation}
  \label{eq:rss}
  \mathrm{RSS}_w\coloneqq\int\left[\hat{w}(t) - w(t)\right]^2\mathrm{d}t
\end{equation}
RSS cannot compare a discrete function with a continuous one.  Even with two similar functions, RSS is not sensitive to timing.  In Fig.~\ref{fig:l2}, RSS of  $a-b_{1}$ and $a-b_{2}$ are both 0.25.  Intuitively $b_1$ is closer to $a$ than $b_2$ and RSS fails to capture such a difference.
\begin{figure}[H]
  \centering
  \scalebox{0.4}{\input{figures/tab.pgf}}
  \caption{\label{fig:l2} $b_1$ and $b_2$ have the same $\mathrm{RSS}=0.25$ to $a$, but $b_1$ is closer in timing to $a$.}
\end{figure}

The PE space is sparse. RSS metric of $\tilde{\phi}(t)$ and $\hat{\phi}(t)$ suffers from the ambiguity in Fig.~\ref{fig:l2}: functions having different timing properties are indistinguishable by $\tilde{\phi}(t)$-$\hat{\phi}(t)$ RSS.  Unless specified otherwise, $\mathrm{RSS}$ refers to $\mathrm{RSS}_w$ in~\eqref{eq:rss}.

\subsubsection{Wasserstein Distance}
\label{sec:W-dist}

\input{fom}

Waveform analysis algorithms are investigated in categories of heuristics, deconvolution, neuron network and regression by the discussed evaluation criteria.

\subsection{Heuristic Methods}
By directly extracting waveform features, heuristic methods are straightforward to implement and thus widely used. 

\subsubsection{Waveform Shifting}
\label{sec:shifting}
A threshold $V_\mathrm{th}$ is selected with respect to electronics noise in the waveforms. Waveform entries $w(t_i)$ exceeding $V_\mathrm{th}$ are selected and shifted by $\Delta t$ according to the shape of SPE response $V_\mathrm{PE}(t)$ in \eqref{eq:dayaspe}. $\hat{q}_i$ are made proportional $w(t_i)$ and normalized to total charge by integration of the waveform.  Waveform shifting does minimal analysis and serves as a baseline method.  The demonstration in Fig.~\ref{fig:shifting} gives $D_w = \SI{7.89}{ns}$.

\begin{figure}[H]
  \begin{subfigure}{0.5\textwidth}
    \centering
    \scalebox{0.36}{\input{figures/threshold.pgf}}
    \caption{\label{fig:shifting}Waveform shifting: $\mathrm{RSS}=\SI{2283.5}{mV^2},D_w=\SI{3.52}{ns},\Delta t_0=\SI{7.89}{ns}$}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \scalebox{0.36}{\input{figures/findpeak.pgf}}
    \caption{\label{fig:peak}Peak finding: $\mathrm{RSS}=\SI{2518.5}{mV^2},D_w=\SI{4.59}{ns},\Delta t_0=\SI{8.70}{ns}$}
  \end{subfigure}
  \caption{Heuristic methods demonstrations.}
\end{figure}

\subsubsection{Peak Finding}
\label{sec:findpeak}

In the peak finding method, local maxima are extracted in Savitzky-Golay smoothed waveforms.  A minus shift $\Delta t$ is applied to peak positions $\hat{t}_i$ to minimize $D_w$ considering SPE response $V_\mathrm{PE}(t)$ in \eqref{eq:dayaspe}. The reconstructed charges $\hat{q}_i$ of $\hat{t}_i$ are proportional to the peak voltages, which are normalized to the total charge by integration of the waveform.  This works well for lower PE counts. When PE's pile-up closely, peaks overlaps and cannot be extracted reliably. A demonstration shown in Fig.~\ref{fig:peak} has $D_w = \SI{4.59}{ns}$. 

\subsection{Deconvolution}
While heuristic methods extract only a timing shift, deconvolution methods exploit the full SPE shape.  Deconvolution can accommodate overshoots and is selected by JUNO prototype to measure charges~\cite{zhang_comparison_2019}.  Smoothing is important because deconvolution does not model the white noise.

\subsubsection{Fourier Deconvolution}

Take Fourier transform $\mathcal{F}$ of \eqref{eq:1},
\begin{equation}
  \label{eq:fourier}
  \mathcal{F}[w] = \mathcal{F}[\tilde{\phi}]\mathcal{F}[V_\mathrm{PE}] + \mathcal{F}[\epsilon],
\end{equation}
therefore
\begin{equation}
  \label{eq:fdconv}
  \mathcal{F}[\tilde{\phi}] = \frac{\mathcal{F}[w]}{\mathcal{F}[V_\mathrm{PE}]} - \frac{\mathcal{F}[\epsilon]}{\mathcal{F}[V_\mathrm{PE}]}.
\end{equation}
The noise term $\epsilon$ is unknown, but can be suppressed by a threshold $V_\mathrm{th}$ and a low pass filter if $V_\mathrm{PE}(t)$ is smooth.  The filter can be realized by a multiplier $R$ in the frequency domain. The estimator $\hat{\phi}(t)$ takes the over-threshold part of inverse Fourier transformationed function of $t$ and is defined in 3 steps,
\begin{equation}
  \label{eq:fdconv2}
  \begin{aligned}
    \hat{\phi}''(t) & = \mathcal{F}^{-1}\left[\frac{R \mathcal{F}[w]}{\mathcal{F}[V_\mathrm{PE}]}\right](t) \\
    \hat{\phi}'(t) & = \hat{\phi}''(t) I(\hat{\phi}''(t) - V_\mathrm{th}) \\
    \hat{\phi}(t) & = \hat{Q}\hat{\phi}_*'(t) \\
  \end{aligned}
\end{equation}
where $I(x)$ is the indicator function,
\begin{equation}
  \label{eq:id}
  I(x) = \left\{
    \begin{array}{ll}
      1 & \mbox{, if $x\ge0$}, \\
      0 & \mbox{, otherwise}
    \end{array}
    \right.
\end{equation}
$\hat{Q}$ is the total relative charge,
\begin{equation}
  \label{eq:Q}
  \hat{Q} = \frac{\int w(t) \mathrm{d}t}{\int V_\mathrm{PE}(t) \mathrm{d}t},
\end{equation}
and $\hat{\phi}_*'(t)$ is the normalized $\hat{\phi}'(t)$. Figure~\ref{fig:fd} illustrates the algorithm with an example having $D_w = \SI{1.72}{ns}$. 

\begin{figure}[H]
  \begin{subfigure}{0.5\textwidth}
    \centering
    \scalebox{0.36}{\input{figures/fftrans.pgf}}
    \caption{\label{fig:fd}Fourier deconvolution: $\mathrm{RSS}=\SI{239.5}{mV^2},D_w=\SI{1.72}{ns},\Delta t_0=\SI{2.43}{ns}$}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \scalebox{0.36}{\input{figures/lucyddm.pgf}}
    \caption{\label{fig:lucy}Lucy direct demodulation: $\mathrm{RSS}=\SI{68.54}{mV^2},D_w=\SI{1.01}{ns},\Delta t_0=\SI{2.76}{ns}$}
  \end{subfigure}
  \caption{Deconvolution demonstrations.}
\end{figure}


\subsubsection{Lucy Direct Demodulation}

Lucy-Richardson direct demodulation (LucyDDM) uses non-linear iteration to calculate deconvolutio, and is used in astronomy \cite{li_richardson-lucy_2019}, image processing, and many other fields.  Viewing $V_{\mathrm{PE}*}(t-s)$ as a conditional probability distribution $P(t|s)$ of amplified electron time $t$ given PE occuring at $s$, by the Bayesian rule,
\begin{equation}
  \label{eq:lucy}
  \tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s) = P(t,s) = \tilde{w}_*(t)P(s|t)
\end{equation}
where $P(t,s)$ is the joint distribution of amplified electron and PE timings.  Therefore,
\begin{equation}
  \label{eq:ptt}
  P(s|t) = \frac{\tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s)}{\tilde{w}_*(t)} = \frac{\tilde{\phi}(s) V_{\mathrm{PE}}(t-s)}{\int\tilde{\phi}(s') V_{\mathrm{PE}}(t-s')\mathrm{d}s'}.
\end{equation}
where on the right hand side, normalization constants cancel each other. Construct $\phi_*$ recurrence relation by replacing $\tilde{w}_*$ with the observed normalized waveform $w_*$,
\begin{equation}
  \label{eq:iter}
  \begin{aligned}
    \tilde{\phi}_*(s) & = \int P(s|t) w_*(t)\mathrm{d}t =  \int \frac{\tilde{\phi}(s) V_{\mathrm{PE}}(t-s)}{\int\tilde{\phi}(s') V_{\mathrm{PE}}(t-s')\mathrm{d}s'} w_*(t) \mathrm{d}t \\
    \implies \hat{\phi}^{n+1}(s) & = \int \frac{\hat{\phi}^n(s) V_{\mathrm{PE}}(t-s)}{\int\hat{\phi}^n(s') V_{\mathrm{PE}}(t-s')\mathrm{d}s'} w(t) \mathrm{d}t.
  \end{aligned}
\end{equation}
Like Fourier deconvolution in \eqref{eq:fdconv2}, this method thresholds and scales the converged $\hat{\phi}^{n}$.  The final scaling eliminates the need for keeping $\hat{\phi}^{n}$ normalized in \eqref{eq:iter}.  LucyDDM constructs $\hat{\phi}$ to be positive, making it more resilient to noise.  Figure~\ref{fig:lucy} shows a LucyDDM example with $D_w = \SI{1.01}{ns}$.

\subsection{Convolutional Neural Networks}

% Network structure
Advances in neural networks have brought breakthroughs in various domains like computer vision and natural language processing. As an efficient composition of weighted additions and pointwise non-linear functions, neural networks have prevailed against many traditional algorithms. We introduce a multi-layered convolutional neural network~(CNN) to process time-sensitive PEs $\tilde{\phi}(t)$ from waveforms $w(t)$ in \eqref{eq:1}.  Because $V_\mathrm{PE}(t)$ is localized, the convolutional widths are selected accordingly.

Although advanced structures like residual connection and strategies like batch-norm alleviated training convergence problems for deep networks, the pulse-shape and universality of $V_\mathrm{PE}(t)$ for all the PE signals permit patterns to be recognized in only a few layers.  We chose a shadow network structure of 5 layers~(Fig.~\ref{fig:struct}) to cut the computation in massive experimental data.  Deeper structures give diminishing returns.

The workflow of data processing consists of training and predicting. Training is to find an efficient mapping from detector waveforms $w(t)$ to PE $\tilde{\phi}(t)$ with backpropagation methods by supervised learning.  The learning is achieved by minimizing the Wasserstein distance loss $W_d$ between the truth $\tilde{\phi}$ and predicted $\hat{\phi}(t)$.  As discussed in Section~\ref{sec:W-dist}, $W_d$ is chosen to handle the PE sparsity and output proximity judgement in training iterations.  KL divergences discussed in Section~\ref{sec:pseudo} is a candidate but it is still to be explored to implement in a trainable form for backpropagation.  RSS is less timing sensitive.  The evolution of Wasserstein distance during training is shown in figure~\ref{fig:loss}. 

\begin{figure}[H]
  \begin{subfigure}{0.35\textwidth}
    \centering
    \begin{adjustbox}{width=0.65\textwidth}
      \input{model}
    \end{adjustbox}
    \caption{\label{fig:struct} CNN structure}
  \end{subfigure}
  \begin{subfigure}{0.6\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/epoch.pgf}}
    \caption{\label{fig:loss} Evolution of loss}
  \end{subfigure}
  \caption{\label{fig:CNN} Training of a CNN}
\end{figure}

The product of network training is a function that creates PE outputs $\hat{\phi}(t)$ from waveform inputs $w(t)$.  Since $W_d$ disregards normalization, $\hat{\phi}$ is scaled to $\hat{Q}\hat{\phi}_*$ following~\eqref{eq:fdconv2}. A demonstration shown in Fig.~\ref{fig:cnn} has $D_w = \SI{0.64}{ns}$. 

The result of $\hat{\phi}(t)$ when $\mu=5, \tau=\SI{20}{ns}, \sigma=\SI{5}{ns}$ is shown in Fig.~\ref{fig:cnn-npe}. The left panel is the boxplot which shows the $D_d$ percentile vs $N_{pe}$, and the right panel is the overall histogram of $D_d$. 

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/takara.pgf}}
    \caption{\label{fig:cnn}$\mathrm{RSS}=\SI{62.77}{mV^2},D_w=\SI{0.64}{ns},\delta=\SI{2.86}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/takarachargestats.pgf}}
    \caption{\label{fig:cnn-npe} $D_d$ vs $N_{pe}$ Histogram \& Boxplot}
  \end{subfigure}
  \caption{CNN when $\mu=5, \tau=\SI{20}{ns}, \sigma=\SI{5}{ns}$}
\end{figure}

\subsection{Regression Analysis}
With waveform generative model in~\eqref{eq:1}, regression analysis is the best for waveform analysis.  Computational complexity hindered regression to be used in huge volumes of raw data.  With the advancement of big data infrastructure and research on sparsity, the statistical advantage of regression becomes more pronounced.

\subsubsection{Gradient Descent}
In (\ref{eq:w-hat}), replace $\hat{N}$ with a fixed sample size $N_s$ and $\hat{t}_i$ with a set of equidistant timings $t'_1, \ldots, t'_{N_s}$,
\begin{equation}
  \label{eq:gd}
  w'(t) = \sum_{i=1}^{N_s}q'_iV_\mathrm{PE}(t-t'_i)
\end{equation}
Construct estimators $\hat{q}_i$ by minimizing RSS of $w'(t)$ and $w(t)$,
\begin{equation}
  \label{eq:gd-q}
  \hat{q}_1, \ldots, \hat{q}_{N_s} = \arg \underset{q'_1, \ldots, q'_{N_s}}{\min} \mathrm{RSS}\left[w'(t),w(t)\right].
\end{equation}
When $\{t'_i\}$ is dense enough, $\{\hat{q}_i\}$ determines the inferred PE distribution $\hat{\phi}(t)$,
\begin{equation}
  \label{eq:gd-phi}
  \hat{\phi}(t) = \sum_{i=1}^{N_s}\hat{q}_i\delta(t-t'_i).
\end{equation}

It is tempting to replace the dense $\{t'_i\}$ grid in \eqref{eq:gd} with a length-varying set of sparse PEs. However, because the truth $N_\mathrm{PE}$ is unknown, that formulates an explicit trans-dimensional model.  Although worth trying, model selection will complicate the optimization and is beyond the scope of this study.

The optimizer is realized with limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm with bound constraint~\cite{byrd_limited_1995}.  It can impose constraints of $q'_i > 0$. The demonstration~\ref{fig:fitting} here shows a result with $D_w = \SI{0.71}{ns}$.

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/xiaopeip.pgf}}
    \caption{\label{fig:fitting}$\mathrm{RSS}=\SI{44.97}{mV^2},D_w=\SI{0.71}{ns},\delta=\SI{3.14}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/xiaopeipchargestats.pgf}}
    \caption{\label{fig:fitting-npe} $D_d$ vs $N_{pe}$ Boxplot \& Histogram}
  \end{subfigure}
  \caption{Gradient descent when $\mu=5, \tau=\SI{20}{ns}, \sigma=\SI{5}{ns}$}
\end{figure}

The result of $\hat{q}$ when $\mu=5, \tau=\SI{20}{ns}, \sigma=\SI{5}{ns}$ is shown in Fig.~\ref{fig:fitting-npe}. The left panel is the boxplot which shows the $D_d$ percentile vs $N_{pe}$, and the right panel is the overall histogram of $D_d$. 

\subsubsection{Markov Chain Monte Carlo}
The optimization problem in \eqref{eq:gd-q} is in general not convex and may be stuck in local minima.  Markov chain Monte Carlo~(MCMC) has the potential to overcome this difficulty.

Hamiltonian Monte Carlo~(HMC) is an efficient MCMC for for high dimensional distributions~\cite{neal_mcmc_2012} such as the posterior of $\{q'_i\}$.  The posterior is combined with a $\{q'_i\}$ prior conditioned on $t_0$ to form a hierarchical model,
\begin{equation}
  \begin{aligned}
    t_{0} &\sim \mathrm{Uniform}(0, \overline{t_0}) \\
    p_i &= \mu \int_{t'_i-\frac{\Delta t'}{2}}^{t'_i+\frac{\Delta t'}{2}} \phi(t' - t_0)\mathrm{d}t' \approx \mu\phi(t' - t_0)\Delta{t'}  \\
    z_i &\sim \mathrm{Binomial}(p_i) \\
    q'_{i,0}&=0\\
    q'_{i,1}& \sim \mathrm{Normal}(1, \sigma_q)\\
    q'_i &= q'_{i,z_i}\\
    w'(t) & = \sum_{i=1}^{N_s}q'_iV_\mathrm{PE}(t-t'_i)\\
    w(t) &\sim \mathrm{Normal}(w'(t), \sigma_\epsilon)
  \end{aligned}
  \label{eq:mixnormal}
\end{equation}
where $t_{0}$ is sampled from a uniform prior with upper bound $\overline{t_{0}}$, and $p_i$ the expection of a PE hitting $(t_{i} - \frac{\Delta t'}{2}, t_{i} + \frac{\Delta t'}{2})$ when $\Delta t'$ is small enough. $q'_i$ is a mixture of 0 (no PE) and normally distributed $q'_{i,1}$ (1 PE).  The inferred waveform $w'(t)$ differs from observation $w(t)$ by a white noise $\epsilon(t) \sim \mathrm{Normal}(0, \sigma_\epsilon)$.  When $\Delta{t'} \to 0$, formulation~\eqref{eq:mixnormal} is equivalent to~\eqref{eq:1} up to a shift by $t_0$.  Instead of using $N_\mathrm{PE}$ to govern the number of pairs $(t_i, q_i)$, the trans-dimensionality is encoded implicitly by $z_i$ to turn on or off $q'_i$ .

Posterior samples of $t_0$ and $\{q'_i\}$ are generated by HMC.   Ergodicity of HMC prevents local maxima from trapping the sampler.  We use NumPyro~\cite{phan2019composable}, a graceful probabilistic programming library, to drive the HMC.  Instead of fixing to 0, $q'_{i,0}$ is modeled as a narrow $\mathrm{Normal}(0, \sigma_0)$ to adapt to NumPyro.

$\hat{t}_0$ and $\hat{q}_i$ are constructed as the mean estimators of posterior samples.  Unlike the pseudo-likelihood $\hat{t}_\mathrm{KL}$ discussed in Section \ref{sec:pseudo}, $\hat{t}_0$ is a genuine Bayesian estimator.  Construct $\hat{\phi}(t)$ as,
\begin{equation}
  \label{eq:mcmc-phi}
  \hat{\phi}(t) = \sum_{i=1}^{N_s}\hat{q}_i\delta(t-t'_i).
\end{equation}

The distribution of $\Delta{t_0} = \hat{t}_0 - t_0$ is shown in Fig.~\ref{fig:mcmc-t0hist} and $W_d$ when $\mu=5, \tau=\SI{20}{ns}, \sigma=\SI{5}{ns}$ is shown in Fig.~\ref{fig:mcmc}.
\begin{figure}[H]
    \centering
    \resizebox{0.5\textwidth}{!}{\input{figures/mcmct0hist.pgf}}
    \caption{\label{fig:mcmc-t0hist} $\Delta t_{0}$ Histogram, $\mu=5, \tau=\SI{20}{ns}, \sigma=\SI{5}{ns}$, method is MCMC}
\end{figure}

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/mcmc.pgf}}
    \caption{\label{fig:mcmc}MCMC: $\mathrm{RSS}=\SI{}{mV^2},D_w=\SI{}{ns},\delta=\SI{}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/mcmcchargestats.pgf}}
    \caption{\label{fig:mcmc-npe} $D_d$ vs $N_{pe}$ Boxplot \& Histogram}
  \end{subfigure}
  \caption{MCMC results when $\mu=5, \tau=\SI{20}{ns}, \sigma=\SI{5}{ns}$}
\end{figure}

\subsubsection{Fast Bayesian Matching Pursuit}

Fast Bayesian Matching Pursuit (FBMP) is a parameters estimation method for sparse linear models~\cite{schniter_fast_nodate}. The linear sparse model in waveform analysis is given by \eqref{eq:fbmpmodel}: 

\begin{align}
    w(t) &= F_{w}(q) = \vec{A}\cdot \vec{q} \label{eq:fbmpmodel} \\
    \hat{\vec{s}}_{*} &\triangleq \mathrm{argmax}\,p(\vec{s}|\vec{y}) \label{eq:map}
\end{align}
in which $A$ is a matrix derived from SPE response~\eqref{eq:dayaspe}. $\vec{q}$ is the charge vector of PE with length $N$, in which most elements are zeros. FBMP algorithm select model before estimation of $q$. For a certain model, $\vec{s}$ is a model selection vector that contains only 0 and 1. Only if $s_{n}=1$, $q_{n}\neq0$, where $\{q_{n}\}_{n=0}^{N-1}$. Implementing the maximum a posteriori (MAP) model-vector estimate (given by~\eqref{eq:map}), a repeated greedy search (RGS) is performed to get a set of dominant models, $\mathcal{S}_{*}$, in the parameter space of $\vec{s}$. RGS will stop after a controlled number of steps, as we believe the search for dominant models set is sufficient enough. Additionally, a linear algebra trick reduces the complexity of the algorithm \cite{schniter_fast_nodate}. Finally, a set of dominant models $\mathcal{S}_{*}$ and its corresponding set of posterior probability, as well as estimations of $\vec{q}$ for each model in $\mathcal{S}_{*}$, are collected to be the approximation of sparse posterior estimation of $\vec{q}$. We use the charge estimation of the maximum posterior probability model as the waveform analysis result. 

Figure~\ref{fig:fbmp} shows one of the FBMP results. We can see the origin waveform $w(t)$ and reconstructed waveform $\hat{w}(t)$ are very similar. The reconstructed and truth $\hat{q}$ are also similar. 

\begin{figure}[H]
    \centering
    \scalebox{0.4}{\input{figures/demoe2c0.pgf}}
    \caption{\label{fig:fbmp} FBMP Demo, $\mathrm{RSS}=\SI{24.30}{mV^2},D_w=\SI{0.48}{ns},\delta=\SI{2.91}{ns}$}
\end{figure}

The result of $\hat{q}$ when $\mu=5, \tau=\SI{20}{ns}, \sigma=\SI{5}{ns}$ is shown in Fig.~\ref{fig:fbmp-npe}. The left panel is the boxplot which shows the $D_d$ percentile vs $N_{pe}$, and the right panel is the overall histogram of $D_d$. 

\begin{figure}[H]
    \centering
    \resizebox{0.5\textwidth}{!}{\input{figures/fbmpchargestats.pgf}}
    \caption{\label{fig:fbmp-npe} $D_d$ vs $N_{pe}$ Boxplot \& Histogram, $\mu=5, \tau=\SI{20}{ns}, \sigma=\SI{5}{ns}$, method is FBMP}
\end{figure}

It is noted that one of the deconvolution methods can be used for FBMP initialization, where we reduce $N$ regarding deconvolution results. In the test of FBMP, we use LucyDDM as the initializer. 

\subsection{Program Efficiency}

The efficiency of 5 good performance methods on our simulation dataset show in table~\ref{fig:efficiency}. The CNN is faster than Lucy direct demodulation and FBMP, and much faster than fitting, while Lucy direct demodulation and FBMP are commensurable. When we reduce the accuracy requirement of these methods except CNN, such as decreasing the number of iteration (for LucyDDM and Fitting) or the sampling amount (for FBMP and MCMC), the time these methods consume will decrease. The time consumed by FBMP contains the initialization time of LucyDDM initializer. 

\begin{table}[H]
    \centering
    \caption{\label{fig:efficiency} Reconstruction Efficiency.}
    \begin{tabular}{cc}
        \hline
        & Performance/$10^{5}$Waveform \\
        \hline
        CNN & 7.5s (GPU\tablefootnote{one graphics card of NVIDIA\textsuperscript{\textregistered} Tesla\textsuperscript{\textregistered} K80.}) \\
        FBMP & 432.7s (CPU\tablefootnote{100 CPU cores of AMD EYPC\texttrademark\ 7702}) \\
        Gradient Descent & 1072.5s (CPU) \\
        LucyDDM & 362.7s (CPU) \\
        MCMC & 15539.1s (CPU) \\
        \hline
    \end{tabular}
\end{table}
\hspace{4mm}

% section Algorithm (end)
