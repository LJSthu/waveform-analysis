\section{Algorithms and performance}
\label{sec:algorithm}

The goal of waveform analysis is to construct estimators $\hat{t}_i$ and $\hat{q}_i$ from waveform $w(t)$, in which the output indices $i$ is from 1 to $\hat{N}$. Figure~\ref{fig:io} illustrates the input $w(t)$ and the outputs $\bm{\hat{t}}, \bm{\hat{q}}$, where the bold face $\bm{\hat{t}}$ denotes the vector $\hat{t}_i$. 
\begin{figure}[H]
  \centering
  \begin{subfigure}{.45\textwidth}
    \resizebox{\textwidth}{!}{\input{figures/wave.pgf}}
    \caption{\label{fig:input} Input $w(t)$}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
    \resizebox{\textwidth}{!}{\input{figures/charge.pgf}}
    \caption{\label{fig:output} Output $\bm{\hat{t}}, \bm{\hat{q}}$}
  \end{subfigure}
  \caption{\label{fig:io}PMT waveform input~\subref{fig:input} and the output for the inferred timings $t_i$ and charges $q_i$ of the PEs~\subref{fig:output} in our waveform analysis. }
\end{figure}

$\sigma_q$, the standard deviation of $q_i$, is $\num{\sim 0.4}$. Such a spread of $q_i$ makes $\hat{t}_i$ ambiguous. For example, the case of $\hat{q}_i=1.6$ can be generated by 1 or 2~PEs, or at a lower probability, even 3~PEs at $t_i$. Single PE of $q_i=1$ may be inferred as $\hat{t}_i$ and $\hat{t}_{i+1}$ with $\hat{q}_i=\hat{q}_{i+1}=0.5$ and mis-interpreted as 2~PEs. Consequently, $\hat{N}$ fails to estimate of $N_\mathrm{PE}$, causing a problem when using $\bm{\hat{t}}$ and $\bm{\hat{q}}$ simultaneously for timing measurement. 

\subsection{Evaluation criteria}
\label{sec:criteria}
Facing the ambiguity of $t_i$ and $q_i$, we develop a set of evaluation criteria to assess the algorithms' performance. 

\subsubsection{Kullback-Leibler divergence}
\label{sec:pseudo}

One idea to plug $\hat{t}_i$ into \eqref{eq:2} is to interpret $\hat{q}_i$ as weights,
\begin{equation}
  \label{eq:pseudo}
  \hat{t}_\mathrm{KL} = \arg\underset{t_0}{\max} \prod_{i=1}^{\hat{N}} \left[\phi(\hat{t}_i-t_0)\right]^{\hat{q}_i}
\end{equation}
where $\hat{t}_\mathrm{KL}$ is not a valid, but a pseudo-likelihood estimator. However, it can be understood as the Kullback-Leibler divergence~\cite{kullback_information_1951} between an inferred and the true light curves.

$\bm{\hat{t}}$ and $\bm{\hat{q}}$ give a light curve estimator $\hat{\phi}(t)$,
\begin{equation}
  \label{eq:lc}
  \hat{\phi}(t) = \sum_{i=1}^{\hat{N}} \hat{q}_i\delta(t-\hat{t}_i)
\end{equation}
up to a normalization factor, where $\delta(t)$ is the Dirac delta function to model discrete values. Consider the non-normalized KL divergence~\cite{mihoko_robust_2002} from $\hat{\phi}(t)$ to $\phi(t-t_{0})$,
\begin{equation}
  \begin{aligned}
    D_\mathrm{KL}\left[\hat{\phi}(t) \parallel \phi(t-t_0)\right] & =\int \left[\hat{\phi}(t) \log\frac{\hat{\phi}(t)}{\phi(t-t_0)} + \phi(t-t_0) - \hat{\phi}(t) \right]\mathrm{d}t \\
    & = - \int \hat{\phi}(t) \log\phi(t-t_0)\mathrm{d}t + \int \left[\hat{\phi}(t) \log\hat{\phi}(t) + \phi(t) - \hat{\phi}(t) \right]\mathrm{d}t \\
    & = - \sum_{i=1}^{\hat{N}}\int \hat{q}_i\delta(t-\hat{t_i}) \log\phi(t-t_0)\mathrm{d}t + C \\
    & = -\log \prod_{i=1}^{\hat{N}} \left[\phi(\hat{t}_i-t_0)\right]^{\hat{q}_i} + C
  \label{eq:kl}
  \end{aligned}
\end{equation}
where $C$ is a constant regarding $t_0$. Comparing with \eqref{eq:pseudo}, it is evident that
\begin{equation}
  \label{eq:kl2}
  \hat{t}_\mathrm{KL} = \arg\underset{t_0}{\min}~D_\mathrm{KL}\left[\hat{\phi}(t) \parallel \phi(t-t_0)\right].
\end{equation}
Like $\sigma_\mathrm{ALL}$, we use the standard deviation of $\hat{t}_\mathrm{KL} - t_0$ for a 
batch of waveforms, denoted as $\sigma_\mathrm{KL}$, to measure the resolutions of waveform analysis methods.

\subsubsection{Residual sum of squares}
\label{sec:rss}

Following~\eqref{eq:1} and~\eqref{eq:lc}, we construct an estimator of waveform,
\begin{equation}
  \label{eq:w-hat}
  \hat{w}(t) = \sum_{i=1}^{\hat{N}}\hat{q}_i V_\mathrm{PE}(t-\hat{t}_i) = \hat{\phi}(t) \otimes V_\mathrm{PE}(t).
\end{equation}
Sum of squares of the residual $\hat{w}(t) - w(t)$ is a natural $\ell_2$-metric inspired by the Gaussian noise term in \eqref{eq:1},
\begin{equation}
  \label{eq:rss}
  \mathrm{RSS}_w\coloneqq\int\left[\hat{w}(t) - w(t)\right]^2\mathrm{d}t.
\end{equation}
RSS cannot compare a discrete function with a continuous one. Even with two similar functions, RSS is not sensitive to timing. In figure~\ref{fig:l2}, the RSS of $a-b_{1}$ and $a-b_{2}$ are both 0.25. Intuitively $b_1$ is closer to $a$ than $b_2$, and RSS fails to capture such a difference.
\begin{figure}[H]
  \centering
  \resizebox{\textwidth}{!}{\input{figures/tab.pgf}}
  \caption{\label{fig:l2} $b_1$ and $b_2$ have the same $\mathrm{RSS}$ ($=0.25$) to $a$, but $b_1$ is closer in timing to $a$.}
\end{figure}

The PE space is sparse. The RSS metric of $\tilde{\phi}(t)$ and $\hat{\phi}(t)$ suffers from the ambiguity in figure~\ref{fig:l2}: functions with different timing properties are indistinguishable by $\tilde{\phi}(t)$-$\hat{\phi}(t)$ RSS. Unless specified otherwise, $\mathrm{RSS}$ refers to $\mathrm{RSS}_w$ in~\eqref{eq:rss}.

\subsubsection{Wasserstein distance}
\label{sec:W-dist}

\input{fom}

We investigate waveform analysis algorithms in heuristics, deconvolution, neuron network, and regression by the discussed evaluation criteria.

\subsection{Heuristic methods}
By directly extracting waveform features, heuristic methods are straightforward to implement and thus widely used. 

\subsubsection{Waveform shifting}
\label{sec:shifting}
We set a threshold $V_\mathrm{th}$ to electronics noise in the waveforms. Waveform entries $w(t_i)$ exceeding $V_\mathrm{th}$ are selected and shifted by $\Delta t$ according to the shape of SPE response $V_\mathrm{PE}(t)$ in equation~\eqref{eq:dayaspe}. $\hat{q}_i$ are normalized to total charge by integration of the waveform $w(t_i)$. Waveform shifting does minimal analysis and serves as a baseline method. The demonstration in figure~\ref{fig:shifting} gives $D_w = \SI{8.72}{ns}$.

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/threshold.pgf}}
    \caption{\label{fig:shifting} $\mathrm{RSS}=\SI{4805.4}{mV^2},D_w=\SI{8.72}{ns},\Delta t_0=\SI{10.74}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/findpeak.pgf}}
    \caption{\label{fig:peak} $\mathrm{RSS}=\SI{317.3}{mV^2},D_w=\SI{2.40}{ns},\Delta t_0=\SI{-1.93}{ns}$}
  \end{subfigure}
  \caption{Heuristic methods demonstrations when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. Waveform shifting is in~\subref{fig:shifting}. Peak finding is in~\subref{fig:peak}.}
\end{figure}

\subsubsection{Peak finding}
\label{sec:findpeak}

In the peak finding method, local maxima are extracted in Savitzky-Golay filter~\cite{savitzky_smoothing_1964} smoothed waveforms. A minus shift $\Delta t$ is applied to peak positions $\hat{t}_i$ to minimize $D_w$ considering SPE response $V_\mathrm{PE}(t)$ in equation~\eqref{eq:dayaspe}. The reconstructed charges $\hat{q}_i$ of $\hat{t}_i$ are proportional to the peak voltages, which are normalized to the total charge by integration of the waveform. This treatment works well for lower PE counts. When PE's pile up closely, peaks overlap and cannot be extracted reliably. Figure~\ref{fig:peak} shows a demonstration with $D_w = \SI{2.40}{ns}$. 

\subsection{Deconvolution}
\label{sec:deconv}
While heuristic methods extract only a timing shift, deconvolution methods exploit the full SPE shape. Deconvolution can accommodate overshoots and becomes an option for the JUNO prototype to measure charges~\cite{zhang_comparison_2019}. Smoothing is necessary because deconvolution does not model the white noise. 

\subsubsection{Fourier deconvolution}

Take Fourier transform $\mathcal{F}$ of \eqref{eq:1}, we have
\begin{equation}
  \label{eq:fourier}
  \begin{aligned}
  \mathcal{F}[w] & = \mathcal{F}[\tilde{\phi}]\mathcal{F}[V_\mathrm{PE}] + \mathcal{F}[\epsilon]\\
  \implies \mathcal{F}[\tilde{\phi}] & = \frac{\mathcal{F}[w]}{\mathcal{F}[V_\mathrm{PE}]} - \frac{\mathcal{F}[\epsilon]}{\mathcal{F}[V_\mathrm{PE}]}.
  \end{aligned}
\end{equation}
The noise term $\epsilon$ is unknown but can be suppressed by a threshold $V_\mathrm{th}$ and a low pass filter if $V_\mathrm{PE}(t)$ is smooth. The filter can be realized by a low pass filter multiplier $R$ in the frequency domain. The estimator $\hat{\phi}(t)$ takes the over-threshold part of the inverse Fourier transform function of $t$, and we define it in 3 steps,
\begin{equation}
  \label{eq:fdconv2}
  \begin{aligned}
    \hat{\phi}''(t) & = \mathcal{F}^{-1}\left[\frac{R \mathcal{F}[w]}{\mathcal{F}[V_\mathrm{PE}]}\right](t) \\
    \hat{\phi}'(t) & = \hat{\phi}''(t) I(\hat{\phi}''(t) - V_\mathrm{th}) \\
    \hat{\phi}(t) & = \hat{\alpha}\hat{\phi}'(t) \\
  \end{aligned}
\end{equation}
where $I(x)$ is the indicator function. Instead of using the total relative charge, we derive $\hat{\alpha}$ from a minimization procedure, 
\begin{equation}
  \begin{aligned}
  \label{eq:id}
  I(x) = \left\{
    \begin{array}{ll}
      1 & \mbox{, if $x\ge0$}, \\
      0 & \mbox{, otherwise}
    \end{array}
    \right.
    \quad
    \hat{\alpha} = \arg \underset{\alpha'}{\min}\mathrm{RSS}\left[\alpha'\hat{\phi}'(t)\otimes V_\mathrm{PE}(t),w(t)\right]. \\
  \end{aligned}
\end{equation}
This $\hat{\alpha}$ scaling process enables us to eliminate the influence of noise. 

Figure~\ref{fig:fd} illustrates the algorithm with an example having $D_w = \SI{2.03}{ns}$. 

\begin{figure}[H]
  \begin{subfigure}{0.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/fftrans.pgf}}
    \caption{\label{fig:fd} $\mathrm{RSS}=\SI{124.7}{mV^2},D_w=\SI{2.03}{ns},\Delta t_0=\SI{-1.16}{ns}$}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/lucyddm.pgf}}
    \caption{\label{fig:lucy} $\mathrm{RSS}=\SI{70.3}{mV^2},D_w=\SI{1.10}{ns},\Delta t_0=\SI{-0.75}{ns}$}
  \end{subfigure}
  \caption{Deconvolution demonstrations when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. Fourier deconvolution is in~\subref{fig:fd}. Lucy direct demodulation is in~\subref{fig:lucy}.}
\end{figure}

\subsubsection{Lucy direct demodulation}

Lucy-Richardson direct demodulation (LucyDDM)~\cite{lucy_iterative_1974} uses non-linear iteration to calculate deconvolution and has a wide application in astronomy \cite{li_richardson-lucy_2019}, image processing, and many other fields. Viewing $V_{\mathrm{PE}*}(t-s)$ as a conditional probability distribution $P(t|s)$ of PMT amplified electron time $t$ given PE occurring at $s$, by the Bayesian rule,
\begin{equation}
  \label{eq:lucy}
  \tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s) = \tilde{\phi}_*(s)P(t|s) = P(t,s) = \tilde{w}_*(t)P(s|t)
\end{equation}
where $P(t,s)$ is the joint distribution of amplified electron and PE timings. Therefore,
\begin{equation}
  \label{eq:ptt}
  P(s|t) = \frac{\tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s)}{\tilde{w}_*(t)} = \frac{\tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s)}{\int\tilde{\phi}_*(s') V_{\mathrm{PE}*}(t-s')\mathrm{d}s'}.
\end{equation}
We construct $\phi_*$ recurrence relation by replacing $\tilde{w}_*$ with the observed normalized waveform $w_*$, where the negative parts are set to 0,
\begin{equation}
  \label{eq:iter}
  \begin{aligned}
    \tilde{\phi}_*(s) & = \int P(s|t) w_*(t)\mathrm{d}t = \int \frac{\tilde{\phi}_*(s) V_{\mathrm{PE}*}(t-s)}{\int\tilde{\phi}_*(s') V_{\mathrm{PE}*}(t-s')\mathrm{d}s'} w_*(t) \mathrm{d}t \\
    \implies \hat{\phi}_*^{n+1}(s) & = \int \frac{\hat{\phi}_*^n(s) V_{\mathrm{PE}*}(t-s)}{\int\hat{\phi}_*^n(s') V_{\mathrm{PE}*}(t-s')\mathrm{d}s'} w_*(t) \mathrm{d}t.
  \end{aligned}
\end{equation}
Like Fourier deconvolution in \eqref{eq:fdconv2}, this method thresholds and scales the converged $\hat{\phi}_*^{n}$. LucyDDM constructs $\hat{\phi}$ to be positive, making it more resilient to noise. Figure~\ref{fig:lucy} shows a LucyDDM example with $D_w = \SI{1.10}{ns}$.

\subsection{Convolutional neural network}

% Network structure
The advances in neural networks have brought breakthroughs in various domains like computer vision~\cite{he_deep_2015} and natural language processing~\cite{vaswani_attention_2017}. As an efficient composition of weighted additions and pointwise non-linear functions, neural networks have prevailed against many traditional algorithms. We introduce a multi-layered convolutional neural network~(CNN) to process time-sensitive PEs $\tilde{\phi}(t)$ from waveforms $w(t)$ in \eqref{eq:1}. Because $V_\mathrm{PE}(t)$ is localized, the convolutional widths are selected accordingly.

Although advanced structures like residual connection and strategies like batch-norm alleviated training convergence problems for deep networks, the pulse-shape, and universality of $V_\mathrm{PE}(t)$ for all the PE signals allow patterns to be recognized in only a few layers. We chose a shallow network structure of 5 layers~(figure~\ref{fig:struct}) to cut the computation in massive experimental data. Deeper structures give diminishing returns.

The workflow of data processing consists of training and predicting. Training involves finding an efficient mapping from detector waveforms $w(t)$ to PE $\tilde{\phi}(t)$ with backpropagation methods by supervised learning. The learning is achieved by minimizing the Wasserstein distance loss $W_d$ between the truth $\tilde{\phi}$ and predicted $\hat{\phi}(t)$. As discussed in section~\ref{sec:W-dist}, $W_d$ is chosen to handle the PE sparsity and output proximity judgment in training iterations. KL divergences discussed in section~\ref{sec:pseudo} is a candidate, but it is still to be explored to implement in a trainable form for backpropagation. RSS is less timing sensitive. Figure~\ref{fig:loss} shows the evolution of Wasserstein distance during training. 

\begin{figure}[H]
  \begin{subfigure}{0.35\textwidth}
    \centering
    \begin{adjustbox}{width=0.65\textwidth}
      \input{model}
    \end{adjustbox}
    \caption{\label{fig:struct} CNN structure}
  \end{subfigure}
  \begin{subfigure}{0.6\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/epoch.pgf}}
    \caption{\label{fig:loss} Evolution of loss}
  \end{subfigure}
  \caption{\label{fig:CNN} Training process of a CNN. A shallow network structure of 5 layers is in~\subref{fig:struct} and the evolution of Wasserstein distance during training is in~\subref{fig:loss}.}
\end{figure}

The product of network training is a function that creates PE outputs $\hat{\phi}(t)$ from waveform inputs $w(t)$. Since $D_w$ disregards normalization, $\hat{\phi}$ is scaled to $\hat{\alpha}\hat{\phi}$ following~\eqref{eq:fdconv2}. Figure~\ref{fig:cnn} shows a demonstration with $D_w = \SI{0.64}{ns}$. 

The result of $\hat{\phi}(t)$ with $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:cnn-npe}. The left panel is the boxplot which shows the $D_d$ percentile vs $N_{\mathrm{PE}}$, and the right panel is the overall histogram of $D_d$. 

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/takara.pgf}}
    \caption{\label{fig:cnn}$\mathrm{RSS}=\SI{10.0}{mV^2},D_w=\SI{0.64}{ns},\Delta t_0=\SI{-3.05}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/takarachargestats.pgf}}
    \caption{\label{fig:cnn-npe} $D_d$ vs $N_{\mathrm{PE}}$ Histogram \& Boxplot}
  \end{subfigure}
  \caption{CNN results when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. A demonstration shows in~\subref{fig:cnn}.}
\end{figure}

\subsection{Regression analysis}
With the generative waveform model in~\eqref{eq:1}, regression analysis is the best for waveform analysis. Although computational complexity hindered regression from being used in vast volumes of raw data, the advancement of big data infrastructure and sparsity research makes the advantage of regression more pronounced.

We replace $\hat{N}$ with a fixed sample size $N_s$ and $\hat{t}_i$ with a fixed vector of timings $t'_i$ in equation~\ref{eq:w-hat} and obtain, 
\begin{equation}
  \label{eq:gd}
  w'(t) = \sum_{i=1}^{N_s}q'_iV_\mathrm{PE}(t-t'_i).
\end{equation}

When $\{t'_i\}$ is dense enough, $\{\hat{q}_i\}$ determines the inferred PE distribution $\hat{\phi}(t)$,
\begin{equation}
  \label{eq:gd-phi}
  \hat{\phi}(t) = \sum_{i=1}^{N_s}\hat{q}_i\delta(t-t'_i).
\end{equation}

It is tempting to replace the dense $\bm{t'}$ grid in equation~\eqref{eq:gd} with a length-varying vector of sparse PEs. However, because the truth $N_\mathrm{PE}$ is unknown, that formulates an explicit trans-dimensional model. Although worth trying, model selection will complicate the optimization and is beyond the scope of this study.

Because the output $\hat{\phi}(t)$ from a deconvolution method~(section~\ref{sec:deconv}) covers locations of non-zero $\bm{q}'$, its discretization can initialize the $\bm{t}'$ grid to reduce $N_s$ and consequently the time consumption.

\subsubsection{Gradient descent}

We construct estimators $\hat{q}_i$ by minimizing RSS of $w'(t)$ and $w(t)$,
\begin{equation}
  \label{eq:gd-q}
  \bm{\hat{q}} = \arg \underset{\bm{q'}}{\min} \mathrm{RSS}\left[w'(t),w(t)\right].
\end{equation}

The optimizer is realized with a limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm with a bound constraint~\cite{byrd_limited_1995}. It can impose constraints of $q'_i > 0$. However, because this method minimizes RSS, it does not scale the converged $\bm{\hat{q}}$. The demonstration~\ref{fig:fitting} here shows a result with $D_w = \SI{0.68}{ns}$.

\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/xiaopeip.pgf}}
    \caption{\label{fig:fitting}$\mathrm{RSS}=\SI{7.63}{mV^2},D_w=\SI{0.68}{ns},\Delta{t_0}=\SI{-3.22}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/xiaopeipchargestats.pgf}}
    \caption{\label{fig:fitting-npe} $D_d$ vs $N_{\mathrm{PE}}$ Boxplot \& Histogram}
  \end{subfigure}
  \caption{Gradient descent results when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. A demonstration shows in~\subref{fig:fitting}.}
\end{figure}

The result of $\hat{q}$ when $\mu=4, \tau_l=\SI{20}{ns}$, and $\sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:fitting-npe}. The left panel is the boxplot which shows the $D_d$ percentile vs $N_{\mathrm{PE}}$, and the right panel is the overall histogram of $D_d$. 

\subsubsection{Markov chain Monte Carlo}
\label{subsec:mcmc}
The optimization problem in equation~\eqref{eq:gd-q} is generally not convex and may be stuck in local minima. Markov Chain Monte Carlo~(MCMC) has the potential to overcome this difficulty.

Hamiltonian Monte Carlo~(HMC) is an efficient MCMC for high dimensional distributions~\cite{neal_mcmc_2012} such as the posterior of $\bm{q'}$. The posterior is combined with a $\bm{q'}$ prior conditioned on $t_0$ to form a hierarchical model,
\begin{equation}
  \begin{aligned}
    t_{0} &\sim \mathrm{Uniform}(0, \overline{t_0}) \\
    p_i &= \mu \int_{t'_i-\frac{\Delta t'}{2}}^{t'_i+\frac{\Delta t'}{2}} \phi(t' - t_0)\mathrm{d}t' \approx \mu\phi(t'_i - t_0)\Delta{t'} \\
    z_i &\sim \mathrm{Binomial}(p_i) \\
    q'_{i,0}&=0\\
    q'_{i,1}& \sim \mathrm{Normal}(1, \sigma_q)\\
    q'_i &= q'_{i,z_i}\\
    w'(t) & = \sum_{i=1}^{N_s}q'_iV_\mathrm{PE}(t-t'_i)\\
    w(t) &\sim \mathrm{Normal}(w'(t), \sigma_\epsilon)
  \end{aligned}
  \label{eq:mixnormal}
\end{equation}
where $t_{0}$ is sampled from a uniform prior with an upper bound $\overline{t_{0}}$, and $p_i$ the expection of a PE hitting $(t_{i} - \frac{\Delta t'}{2}, t_{i} + \frac{\Delta t'}{2})$ when $\Delta t'$ is small enough. $q'_i$ is a mixture of 0 (no PE) and normally distributed $q'_{i,1}$ (1 PE). The inferred waveform $w'(t)$ differs from observable $w(t)$ by a white noise $\epsilon(t) \sim \mathrm{Normal}(0, \sigma_\epsilon)$. When $\Delta{t'} \to 0$, equation~\eqref{eq:mixnormal} is equivalent to equation~\eqref{eq:1} up to a shift by $t_0$. Instead of using $N_\mathrm{PE}$ to govern the number of pairs $(t_i, q_i)$, we encode the trans-dimensionality implicitly by $z_i$ to turn on or off $q'_i$.

Posterior samples of $t_0$ and $\bm{q'}$ are generated by HMC. Ergodicity of HMC prevents a local maxima from trapping the sampler. We use NumPyro~\cite{phan2019composable}, a graceful probabilistic programming library, to drive the HMC. Instead of fixing to 0, we model $q'_{i,0}$ as a narrow $\mathrm{Normal}(0, \sigma_0)$ to adapt to NumPyro.

$\hat{t}_0$ and $\hat{q}_i$ are constructed as the mean estimators of posterior samples. Unlike the pseudo-likelihood $\hat{t}_\mathrm{KL}$ discussed in section~\ref{sec:pseudo}, $\hat{t}_0$ is a genuine Bayesian estimator. We construct $\hat{\phi}(t)$ as,
\begin{equation}
  \label{eq:mcmc-phi}
  \hat{\phi}(t) = \sum_{i=1}^{N_s}\hat{q}_i\delta(t-t'_i).
\end{equation}

$D_w$ with $\mu=4, \tau_l=\SI{20}{ns}$, and $\sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:mcmc-npe}.
\begin{figure}[H]
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/mcmc.pgf}}
    \caption{\label{fig:mcmc}$\mathrm{RSS}=\SI{16.25}{mV^2},D_w=\SI{0.76}{ns},\Delta{t_0}=\SI{-2.48}{ns}$}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/mcmcchargestats.pgf}}
    \caption{\label{fig:mcmc-npe} $D_d$ vs $N_{\mathrm{PE}}$ Boxplot \& Histogram}
  \end{subfigure}
  \caption{MCMC results results when $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$. A demonstration shows in~\subref{fig:mcmc}.}
\end{figure}

\subsubsection{Fast Bayesian matching pursuit}
\label{subsec:fbmp}
We find that $w(t)$ is a Gaussian process subject to $\bm{z}$,
\begin{equation}
    w(t)|\bm{z} \sim \mathcal{GP}\left[\sum_{i|z_i=1}V_\mathrm{PE}(t-t'_i), \sigma_q^2 \sum_{i|z_i=1}V_\mathrm{PE}(t'_i-t)V_\mathrm{PE}(s-t'_i) + \sigma_\epsilon^2 \right],
\end{equation}
where $\bm{z}$ controls which $t'_i$ is in use. In the numerical calculation, the Gaussian process is discretized as a multivariate normal distribution together with $\bm{q}'$
\begin{equation}
\label{eq:mgauss}
\begin{aligned}
    \left.
    \begin{bmatrix}
        \bm{w} \\
        \bm{q}'
    \end{bmatrix}
    \right\vert\bm{z}
    &\sim \mathrm{Normal}\left(
    \begin{bmatrix}
        \bm{V}_\mathrm{PE}\bm{z} \\
        \bm{z}
    \end{bmatrix}, 
    \begin{bmatrix}
        \bm{\Sigma}_z & \bm{V}_\mathrm{PE}\bm{Z} \\
        \bm{Z}\bm{V}_\mathrm{PE}^\intercal & \bm{Z}
    \end{bmatrix}
    \right) \\
    \bm{\Sigma}_z &= \bm{V}_\mathrm{PE}\bm{Z}\bm{V}_\mathrm{PE}^\intercal+\sigma_\epsilon^2\bm{I}
\end{aligned}
\end{equation}
where $\bm{Z}$ is the diagonal matrix of vector $\bm{z}$ controlling $q'_i$. $\bm{I}$ is a unit matrix. Here $\bm{z}$ is not the same as that in section~\ref{subsec:mcmc}: 

\begin{equation}
  \begin{aligned}
    \mu_i &= \mu \int_{t'_i-\frac{\Delta t'}{2}}^{t'_i+\frac{\Delta t'}{2}} \phi(t' - t_0)\mathrm{d}t' \\
    z_i &\sim \mathrm{Poisson}(\mu_i).
  \end{aligned}
  \label{eq:poissonfbmp}
\end{equation}

We denote a set $\mathcal{Z}$ to contain infinite combinations of $\bm{z}$. Only few $\bm{z}$'s give finite probabilities of $\bm{w}$, leading to the Fast Bayesian Matching Pursuit~(FBMP), a parameter estimation method for sparse linear models~\cite{schniter_fast_2008}. FBMP defines a targeted subset $\mathcal{Z}'$ of $\mathcal{Z}$ with only the $\bm{z}$ giving non-negligible $p(\bm{w}|\bm{z})$. The posterior of $\bm{z}$ is approximated as
\begin{equation}
    \label{eq:z}
    \begin{aligned}
    p(\bm{z}|\bm{w}) &= \frac{p(\bm{w}|\bm{z})p(\bm{z})}{\sum_{\bm{z}'\in\mathcal{Z}}p(\bm{w}|\bm{z'})p(\bm{z'})} \\
    &\approx \frac{p(\bm{w}|\bm{z})p(\bm{z})}{\sum_{\bm{z}'\in\mathcal{Z}'}p(\bm{w}|\bm{z'})p(\bm{z'})}.
    \end{aligned}
\end{equation}
The prior $p(\bm{z})$ is given by the Poisson distributions of binned light curve,
\begin{equation}
\begin{aligned}
    p(\bm{z}|t_0, \mu) &= \prod_{i}\mathrm{Poisson}(z_i,\mu_i)
\end{aligned}
\end{equation}
where $\mu$ is the prior expected number of PEs and satisfies the normalization condition in equation~\eqref{eq:normal}. 
\begin{equation}
\label{eq:normal}
\begin{aligned}
  \sum_i \mu_i &= \mu
\end{aligned}
\end{equation}

The nominator logarithm of \eqref{eq:z},
\begin{equation}
    \label{eq:metric}
    \begin{aligned}
        \nu =& \log[p(\bm{w},\bm{z})] = \log[p(\bm{w}|\bm{z})p(\bm{z})] \\
        =& -\frac{1}{2}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})^\intercal\bm{\Sigma}_z^{-1}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})-\frac{1}{2}\log\det\bm{\Sigma}_z-\frac{N_s}{2}\log2\pi \\
        & +\sum_{i}\log{\mathrm{Poisson}(z_i,\mu_i)}
    \end{aligned}
\end{equation}
is treated as a \textit{model selection metric}. A repeated greedy search (RGS) is performed to construct the target set $\mathcal{Z}'$ guided by equation~\eqref{eq:metric}. Additionally, a linear algebraic iteration reduces the complexity of $p(\bm{z}|\bm{w})$ calculation~\cite{schniter_fast_2008}. 

For each $\bm{z} \in \mathcal{Z}'$, from \eqref{eq:mgauss} the expectation estimation of $\bm{q}'$ is, 
\begin{align}
    \hat{\bm{q}}_z = E(\bm{q}'|\bm{w},\bm{z}) &= \bm{z} + \bm{Z}\bm{V}_\mathrm{PE}^\intercal\bm{\Sigma}_z^{-1}(\bm{w}-\bm{V}_\mathrm{PE}\bm{z})
    \label{eq:fbmpcharge}
\end{align}

Finally, all the $\bm{z} \in \mathcal{Z}'$ and their corresponding posterior probabilities $p(\bm{z}|\bm{w})$, and estimations of $\hat{\bm{q}}_z$ are collected to be the total posterior distribution. $\hat{\bm{z}}$ is the PE number in each time bin $\hat{\bm{t}}$. $\hat{\bm{q}}$, which is the total charge in each time bin is estimated by the maximum posterior probability,
\begin{equation}
    \begin{aligned}
        \label{eq:zposterior}
        \hat{\bm{z}} &= \arg \underset{\bm{z}}{\max} p(\bm{z}|\bm{w}) \\
        \hat{\bm{q}} &= \hat{\bm{q}}_{\hat{z}} \\
        \hat{w}(t) &= \sum_{i} \hat{q}_i V_\mathrm{PE}(t-t'_i)
    \end{aligned}
\end{equation}

Figure~\ref{fig:fbmp} shows one of the FBMP results. We can see that the original waveform $w(t)$ and the reconstructed waveform $\hat{w}(t)$ are very similar. The reconstructed and truth $\bm{q}$ are also similar.

\begin{figure}[H]
    \centering
    \resizebox{0.6\textwidth}{!}{\input{figures/demoe2c0.pgf}}
    \caption{\label{fig:fbmp} FBMP Demo, $\mathrm{RSS}=\SI{15.8}{mV^2},D_w=\SI{0.59}{ns},\Delta{t_0}=\SI{-3.51}{ns}$}
\end{figure}

The result of $\hat{q}$ with $\mu=4, \tau_l=\SI{20}{ns}$, and $\sigma_l=\SI{5}{ns}$ is shown in figure~\ref{fig:fbmp-npe}. The left panel is the boxplot which shows the $D_w$ percentiles vs $N_{\mathrm{PE}}$, and the right panel is the overall histogram of $D_w$. 

\begin{figure}[H]
    \centering
    \resizebox{0.5\textwidth}{!}{\input{figures/fbmpchargestats.pgf}}
    \caption{\label{fig:fbmp-npe} $D_w$ vs $N_{\mathrm{PE}}$ Boxplot \& Histogram, $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$, method is FBMP}
\end{figure}

The denominator of equation~\eqref{eq:z} is actually the evidence $p(\bm{w}|t_0, \mu)$, which provides a superior estimator of $\mu$ and $t_0$ to equation~\eqref{eq:pseudo},
\begin{equation}
\begin{aligned}
    \label{eq:bayesianinter}
    \left(\hat{t}_0, \hat{\mu}\right) &= \arg\underset{t_0,\mu}{\max} p(\bm{w} | t_0, \mu) \\
    p(\bm{w}|t_0, \mu) &= \sum_{\bm{z}'\in\mathcal{Z}'}p(\bm{w}|\bm{z}',t_0,\mu)p(\bm{z}'|t_0,\mu) \\
    &= \sum_{\bm{z}'\in\mathcal{Z}'}p(\bm{w}|\bm{z}')p(\bm{z}'|t_0,\mu).
\end{aligned}
\end{equation}
where $p(\bm{w}|z', t_0, \mu) = p(\bm{w}|z')$ because $\bm{w}$ does not depend on $t_0$ or $\mu$ if $z'$ is given. If $t_0$ and $\mu$ are predicted by an event model $p(t_0, \mu | \mathcal{E})$, FBMP is extended to the event reconstruction bypassing the need to save PE $\bm{t}'$ and $\bm{q}'$. While FBMP provides the Bayesian interface of waveform analysis and event reconstruction, it is however out of the scope of this article and will be covered elsewhere.

\subsection{Performance}

% Methods

Figure~\ref{fig:chargesummary} shows the $D_w$ summary of all methods with $\tau_l=\SI{20}{ns}$, and $\sigma_l=\SI{5}{ns}$. The errorbar shows 5 to 95 percentile of Wasserstein distance distribution. 

\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{\input{figures/summarycharge.pgf}}
    \caption{\label{fig:chargesummary} $W_{d}$ for all the methods with light curve $\mu=4, \tau_l=\SI{20}{ns}, \sigma_l=\SI{5}{ns}$}
\end{figure}

It is apparent that FindPeak, Shift, and FFT are second-rated among all eight methods. On time resolution and charge reconstruction which we will discuss in sections~\ref{subsec:timeresolution} and~\ref{subsec:chargereconstruction}, MCMC does not perform well and is time-consuming. So we will only discuss the other four methods below. 

% Figure of Merits

The $D_w$ summary of four methods (LucyDDM, Fitting, CNN, FBMP) is plotted in figure~\ref{fig:wdistsummary}, with all dataset parameters combination. The errorbar shows 5 to 95 percentile of Wasserstein distance distribution. Wasserstein distances of all 4 methods are approaching each other when $\mu$ increases. 

We can see that as method CNN uses Wasserstein distance as loss directly during training, its Wasserstein distance is the smallest. While the Fitting treats RSS as its loss, FBMP obtains the least RSS among four methods (see figure~\ref{fig:rsssummary}) and CNN follows. 
\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{\input{figures/vs-wdist.pgf}}
    \caption{\label{fig:wdistsummary} $D_w$ of methods, error bar 5--95 percentile ($\hat{q}$)}
\end{figure}

\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{\input{figures/vs-rss.pgf}}
    \caption{\label{fig:rsssummary} RSS of methods, error bar 5--95 percentile ($\hat{q}$)}
\end{figure}
