\section{Summary and discussion}
\label{sec:discussion}

In this section, we shall address the burning question: which waveform analysis method should my experiment use?  We surveyed 8 methods, heuristics~(figure~\ref{fig:method}), deconvolution~(figure~\ref{fig:deconv}), neuron network~(figure~\ref{fig:cnn-performance}) and regressions(figures~\ref{fig:dcf}--\ref{fig:fbmp-performance}), from the simplest to the most dedicated.  To make a choice, we shall investigate the light curve reconstruction precisions under different light intensities $\mu$.

\subsection{Performance}

First, we constrain the candidates by time consumption, algorithm category and $D_\mathrm{w}$.  Figure~\ref{fig:chargesummary} shows the $D_w$ and time consumption summary of all 8 methods with the same waveform sample as figure~\ref{fig:method}.
\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{\input{figures/summarycharge.pgf}}
    \caption{\label{fig:chargesummary} Performance of algorithms in terms of $D_\mathrm{w}$ and time consumption, evaluated on the same dataset as figure~\ref{fig:method}. Central points are the average results of $\num[retain-unity-mantissa=false]{1e4}$ waveforms from specific $\mu$ values.  Error bars are 5--95 percentiles.  Fitting stands for direct charge fitting. The time consumed by fitting, MCMC and FBMP include the initialization time of a LucyDDM pre-conditioner.  Time consumption of CNN is measured for inference in two conditions: GPU\protect\footnotemark~(dashed error bars) and CPU\protect\footnotemark~(solid error bars).  Training a CNN is a one-time job, and its cost is not included in the plot.}
\end{figure}
\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{Graphics card of NVIDIA\textsuperscript{\textregistered} Tesla A100. }
\stepcounter{footnote}\footnotetext{One CPU core of AMD EYPC\texttrademark\ 7742. }

The $D_\mathrm{w}$ performance of waveform shifting, peak finding and Fourier deconvolution are suboptimal.  But together with CNN, they are the fastest, because no iteration is involved.  LucyDDM, Fitting and FBMP have $\num{\sim 100}$ iterations and are 2 orders of magnitudes slower.  The number of iterations and samples $N_s$ can be decreased to trade for speed.  MCMC is too expansive and its principle is not too different from FBMP.  We shall focus on CNN, LucyDDM, fitting and FBMP in the following.

% Figure of Merits

The $D_\mathrm{w}$ and RSS dependence on $\mu$ of LucyDDM, Fitting, CNN and FBMP, are plotted in figure~\ref{fig:wdistsummary} and \ref{fig:rsssummary}.  When $\mu$ increases $D_\mathrm{w}$ approach each other, while RSS diverges.  The $D_\mathrm{w}$ decrease-before-increase behavior is observed in section~\ref{sec:cnn} that with large $N_\mathrm{PE}$ the overall PE times dominate.  It is harder to balance $D_\mathrm{w}$ and RSS with larger $N_\mathrm{PE}$, but FBMP achieves the best balance.  It is a interesting question whether FBMP can be approximated by a neuron network for a speed boost.  Notice that in qualitative discussion, large $N_\mathrm{PE}$, large light intensity $\mu$ and large pile-ups are used interchangably.
\begin{figure}[H]
  \begin{subfigure}[b]{\textwidth}
    \resizebox{\textwidth}{!}{\input{figures/vs-wdist.pgf}}
    \caption{\label{fig:wdistsummary}Dependence of Wasserstein distance on light intensity.}
  \end{subfigure}

  \vspace{0.5em}
  \begin{subfigure}[b]{\textwidth}
    \resizebox{\textwidth}{!}{\input{figures/vs-rss.pgf}}
    \caption{\label{fig:rsssummary}Dependence of residual sum of squares on light intensity.}
  \end{subfigure}
  \caption{\label{fig:summary}The dependence of $D_\mathrm{w}$~\subref{fig:wdistsummary} and RSS~\subref{fig:rsssummary} on light intensity $\mu$ for typical Cherenkov (left) and scintillation (right) configurations.  Central points, error bars and method abbreviations have the same meaning as figure~\ref{fig:chargesummary}.  With more pile-ups, $D_\mathrm{w}$ tends to converge while RSS diverges.  The pile-up effect is more significant for Cherenkov case because the time scale of light curve is narrower. }
\end{figure}

CNN and FBMP are the kings of $D_\mathrm{w}$ and RSS.  Because their loss functions are chosen accordingly.  It is therefore informative to study the posterior charge distribution which is not related to the loss function of any method.

\subsection{Charge fidelity and sparsity}

All the discussed mothods output $\hat{q}_i$ as the inferred charge of the PE at $t_i'$.  Evident in figure~\ref{fig:recchargehist}, FBMP considers and almost retains the true charge distribution.  It is the only method modeling PE correctly.

In contrast, the distributions of LucyDDM, Fitting, and CNN are severely distorted.  During the optimization process of $D_\mathrm{w}$ or RSS, the number of $q_i$ is a constant. Many $\hat{q}_i$ are inferred to be fragmented values.  In this view, retaining charge distribution is a manifestation of sparsity.  FBMP has the best sparsity because it chooses a PE configuration $\bm{z}$ before fitting $\hat{q}_i$.  CNN somehow learns the sparsity better than Fitting, although the latter in theory has self-regulated sparsity.  It is interesting to notice, but the mechanism is unknown to us.

\begin{figure}[H]
  \centering
  \resizebox{0.6\textwidth}{!}{\input{figures/recchargehist.pgf}}
  \caption{\label{fig:recchargehist} $\hat{q}_i$ distributions on the same waveform dataset as figure~\ref{fig:method}.  Method abbreviations are defined in figure~\ref{fig:chargesummary}. ChargePDF is the charge distribution introduced for simulation in section~\ref{subsec:spe}. The cut-off near 0 in LucyDDM is an artifect of thresholding in eq.~\eqref{eq:fdconv2}.}
\end{figure}

For large $N_\mathrm{PE}$ waveforms the sparsity condition is by definition lost.  The equivalence of charge fidelity and sparity implies that FBMP breaks for large $N_\mathrm{PE}$ cases where PMTs operates in current mode, as we shall see in the next sections.

\subsection{Inference of incident light}
\label{subsec:timeresolution}

Like figure~\ref{fig:summary}, we show the dependence on $\mu$ of bias~(figure~\ref{fig:biasmethods}) and resolution~(figure~\ref{fig:deltamethods}) for different time estimators in two typical experimental setups.  From figure~\ref{fig:biasmethods}, we see that the $t_0$ estimation biases are all similar to that of $\hat{t}_\mathrm{ALL}$. The considerable bias for the scintillation configureation~(right of figure~\ref{fig:biasmethods}) for small $\mu$ is intrinsic for exponential distributions.

Large light intensity improves the resolution of $\hat{t}_\mathrm{1st}$, as evident in the left of figure~\ref{fig:reso-diff}.  People often argue from such an observation and the difficulties for large pile-ups that it is unnecessary to do waveform analysis when $\mu$ is large.  Comparing figure~\ref{fig:reso-diff} and \ref{fig:deltamethods}, we see it is a myth: all the methods provide magnificently better time resolutions than $\hat{t}_\mathrm{1st}$. For $\mu=30$ the improvement in time resolutions for Cherenkov and scintillation configurations are $2.7$ and $1.5$ times!

\begin{figure}[H]
  \begin{subfigure}[b]{\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/vs-bias.pgf}}
    \caption{\label{fig:biasmethods} Time reconstruction biases.}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/vs-deltamethodsdiv.pgf}}
    \caption{\label{fig:deltamethods} Time reconstruction ratios of resolution $\sigma/\sigma_{\mathrm{ALL}}$.}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/vs-biasmu.pgf}}
    \caption{\label{fig:biasmu} Intensity reconstruction biases $\frac{\hat{\mu} - \mu}{\mu}$.}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \centering
    \resizebox{\textwidth}{!}{\input{figures/vs-deltamethodsdivmu.pgf}}
    \caption{\label{fig:deltamu} Intensity reconstruction ratios of relative resolution $\frac{\sigma_{\hat{\mu}}}{\sqrt{\hat{\mu}}}/\frac{\sigma_\mu}{\sqrt{\mu}}$. }
  \end{subfigure}
  \caption{ALL is the $\hat{t}_\mathrm{ALL}$ estimator defined in eq.~\eqref{eq:2}. LucyDDM, Fitting, CNN use eqs.~\eqref{eq:pseudo} and \eqref{eq:pseudo-mu}.  FBMP has its own natural $\hat{t}_0$ estimator in eq.~\eqref{eq:fbmpcharge}. Error bars are 5--95 percentiles.}
\end{figure}

$\hat{\mu}_Q$ is deteriorated by the randomization process, such as charge distribution and Gaussian white noise in section~\ref{subsec:spe}. 

Figure~\ref{fig:biasmu} shows the bias of $\hat{\mu}$ of all four methods. The bias of FBMP briefly comes from the degeneracy of reconstruction results and the non-ergodic approximation of FBMP. 

Figure~\ref{fig:deltamu} shows the intensity relative resolutions.  For small $\mu$ FBMP gives a much better resolution by correctly modeling charge distributions, see also figure~\ref{fig:recchargehist}.  For larger $\mu$, LucyDDM, Fitting and CNN are similar to $\hat{\mu}_Q$.

% Pedestal & Hardware

\subsection{Prospects}
\label{sec:prospects}

We need to consider pedestals or saturations in real world experiments.  But in this article we choose not to dive into them for simplicity.

We can implement the algorithms in field programmable gate array~(FPGA) commonly found in front-end electronics.  That will not only accelerate waveform analysis, but also perform it online to reduce the volume of data needed for data acquisition.  Waveform analysis inspired data compression will potentially outperform general-purposed algorithms.
